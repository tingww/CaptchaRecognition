{"cells":[{"cell_type":"code","metadata":{"id":"a8YJtT1vZGPp","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mXU7VichQhAZ","colab_type":"code","colab":{}},"source":["!tar zxvf /content/drive/My\\ Drive/Colab\\ Notebooks/train.tgz\n","!pip install wandb"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VnS9Fq9yZeIL","colab_type":"code","colab":{}},"source":["!wandb login"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-_BJrlMdLiBL","colab_type":"text"},"source":["# CAPTCHA Recognition - 6 digits"]},{"cell_type":"code","metadata":{"id":"DL7GU3lIQJjO","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten, Dropout, MaxPooling2D,BatchNormalization\n","\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Cov92DELdjd","colab_type":"text"},"source":["# Parsing Data"]},{"cell_type":"code","metadata":{"id":"tXbxnA_qL-GV","colab_type":"code","colab":{}},"source":["CLASS_NAMES = np.array([])\n","for i in range(10):\n","    CLASS_NAMES =np.append(CLASS_NAMES,chr(ord(\"0\")+i))\n","for i in range(26):\n","    CLASS_NAMES = np.append(CLASS_NAMES,chr(ord(\"A\")+i))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vYwLBL8dQJjX","colab_type":"code","colab":{}},"source":["#functions\n","def parse_label(strIN):\n","    return (strIN==CLASS_NAMES).astype(float)\n","def readimg_to_tensor(fn):\n","    a = tf.io.read_file((fn))\n","        #crop to [60,30,3]\n","    img = tf.io.decode_jpeg(a)\n","    return img"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vAzV-7w5QJjg","colab_type":"code","colab":{}},"source":["#training data 01\n","def prepData01():  \n","  train_dir_data01 = os.path.abspath(os.getcwd())+\"/train/data01_train/\"\n","  data = pd.read_csv(\"./train/data01_train.csv\")\n","  for i in range(len(data)):\n","      data.iloc[i,1] = list(data.iloc[i,1])\n","  arr = np.zeros([6,50000],str)\n","  for j in range(6):\n","      for i in range(len(data)):\n","          arr[j][i] = data.iloc[i,1][j]\n","  data = data.drop(columns= [\"code\"]).join(pd.DataFrame(arr.transpose(),columns = [\"code0\",\"code1\",'code2','code3','code4','code5']))\n","\n","  for i in range(6):\n","      data.iloc[:,i+1] = data.iloc[:,i+1].map(parse_label)\n","\n","  TL = tf.convert_to_tensor(list(data.iloc[:,1].values))\n","  TL2 = tf.convert_to_tensor(list(data.iloc[:,2].values))\n","  TL3 = tf.convert_to_tensor(list(data.iloc[:,3].values))\n","  TL4 = tf.convert_to_tensor(list(data.iloc[:,4].values))\n","  TL5 = tf.convert_to_tensor(list(data.iloc[:,5].values))\n","  TL6 = tf.convert_to_tensor(list(data.iloc[:,6].values))\n","\n","  train_dir = tf.data.Dataset.list_files(train_dir_data01+'*.jpg',shuffle=False) \n","  train_data = train_dir.map(lambda x: readimg_to_tensor(x))\n","  train_label = tf.data.Dataset.from_tensor_slices((TL,TL2,TL3,TL4,TL5,TL6))\n","  train_data01=tf.data.Dataset.zip((train_data,train_label))\n","  validation_split=0.1\n","  split = int(num_elements*validation_split)\n","  train_data_gen=iter(train_data01.shuffle(50000,seed = 102).batch(50000))\n","\n","  img , lb = next(train_data_gen)\n","  img_train = img[:-split]\n","  lb_train = (lb[0][:-split],lb[1][:-split],lb[2][:-split],lb[3][:-split],lb[4][:-split],lb[5][:-split])\n","  img_test = img[-split:]\n","  lb_test = (lb[0][-split:],lb[1][-split:],lb[2][-split:],lb[3][-split:],lb[4][-split:],lb[5][-split:])\n","  return img_train, lb_train, img_test, lb_test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mzURsa87QJjT","colab_type":"code","colab":{}},"source":["#training data 02\n","def prepData02(): \n","  train_dir_data02 = os.path.abspath(os.getcwd())+\"/train/data02_train/\"\n","  data2 = pd.read_csv(\"./train/data02_train.csv\")\n","  for i in range(len(data2)):\n","      data2.iloc[i,1] = list(data2.iloc[i,1])\n","  arr = np.zeros([6,50000],str)\n","  for j in range(6):\n","      for i in range(len(data2)):\n","          arr[j][i] = data2.iloc[i,1][j]\n","  data2 = data2.drop(columns= [\"code\"]).join(pd.DataFrame(arr.transpose(),columns = [\"code0\",\"code1\",'code2','code3','code4','code5']))\n","\n","  #to one hot\n","  for i in range(6):\n","      data2.iloc[:,i+1] = data2.iloc[:,i+1].map(parse_label)\n","\n","  TL = tf.convert_to_tensor(list(data2.iloc[:,1].values))\n","  TL2 = tf.convert_to_tensor(list(data2.iloc[:,2].values))\n","  TL3 = tf.convert_to_tensor(list(data2.iloc[:,3].values))\n","  TL4 = tf.convert_to_tensor(list(data2.iloc[:,4].values))\n","  TL5 = tf.convert_to_tensor(list(data2.iloc[:,5].values))\n","  TL6 = tf.convert_to_tensor(list(data2.iloc[:,6].values))\n","\n","  train_dir2 = tf.data.Dataset.list_files(train_dir_data02+'*.jpg',shuffle=False) \n","  train_data2 = train_dir2.map(lambda x: readimg_to_tensor(x))\n","  train_label2 = tf.data.Dataset.from_tensor_slices((TL,TL2,TL3,TL4,TL5,TL6))\n","  train_data02=tf.data.Dataset.zip((train_data2,train_label2))\n","  validation_split=0.1\n","  num_elements=50000\n","  split = int(num_elements*validation_split)\n","  train_data_gen=iter(train_data02.shuffle(50000,seed = 1).batch(50000))\n","\n","  img , lb = next(train_data_gen)\n","  img_train = img[:-split]\n","  lb_train = (lb[0][:-split],lb[1][:-split],lb[2][:-split],lb[3][:-split],lb[4][:-split],lb[5][:-split])\n","  img_test = img[-split:]\n","  lb_test = (lb[0][-split:],lb[1][-split:],lb[2][-split:],lb[3][-split:],lb[4][-split:],lb[5][-split:])\n","  return img_train, lb_train, img_test, lb_test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T5xSgQVY03hn","colab_type":"code","colab":{}},"source":["#choose data\n","img_train, lb_train, img_test, lb_test= prepData01()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E4sinsEmLUSj","colab_type":"text"},"source":["# Model configeration"]},{"cell_type":"code","metadata":{"id":"JZjWCxXp3-28","colab_type":"code","colab":{}},"source":["import wandb\n","from tensorflow.keras import layers\n","from tensorflow.keras import backend"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QG7fHWxhx0tG","colab_type":"code","colab":{}},"source":["filepath = \"/content/drive/Shared drives/TingWeis_Drive/Colab Notebooks/Saved Model/\"#\"h5s/\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QHcC-7jrzn4H","colab_type":"code","colab":{}},"source":["config_default = {\n","    \"weight_regu\":1e-4,\n","    \"LR\":1e-3,\n","    \"dropout\":0.3,\n","    \"name\":\"Mymodel\",\n","    \"batch\":256\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dFW2e9tfwCit","colab_type":"code","colab":{}},"source":["def block2(x, filters, kernel_size=3, stride=1, conv_shortcut=True, name=None, config = config_default):\n","  \"\"\"A residual block.\n","  Arguments:\n","      x: input tensor.\n","      filters: integer, filters of the bottleneck layer.\n","      kernel_size: default 3, kernel size of the bottleneck layer.\n","      stride: default 1, stride of the first layer.\n","      conv_shortcut: default False, use convolution shortcut if True,\n","        otherwise identity shortcut.\n","      name: string, block label.\n","  Returns:\n","    Output tensor for the residual block.\n","  \"\"\"\n","  initializer = tf.keras.initializers.he_normal( seed = 3)\n","  alpha = config[\"weight_regu\"]  # weight decay coefficient\n","  regularizer = tf.keras.regularizers.l2(alpha)\n","  bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n","\n","  preact = layers.BatchNormalization(epsilon=1.001e-5, name=name + '_preact_bn')(x)\n","  preact = layers.Activation('relu', name=name + '_preact_relu')(preact)\n","\n","  if conv_shortcut:\n","    shortcut = layers.Conv2D(\n","        4 * filters, 1, strides=stride, name=name + '_0_conv', kernel_initializer = initializer,kernel_regularizer = regularizer)(preact)\n","  else:\n","    shortcut = layers.MaxPooling2D(1, strides=stride)(x) if stride > 1 else x\n","\n","  x = layers.Conv2D(\n","      filters, 1, strides=1, use_bias=False, name=name + '_1_conv', kernel_initializer = initializer,kernel_regularizer = regularizer)(preact)\n","  x = layers.BatchNormalization(epsilon=1.001e-5, name=name + '_1_bn')(x)\n","  x = layers.Activation('relu', name=name + '_1_relu')(x)\n","\n","  x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name=name + '_2_pad')(x)\n","  x = layers.Conv2D(\n","      filters,\n","      kernel_size,\n","      strides=stride,\n","      use_bias=False,\n","      name=name + '_2_conv', kernel_initializer = initializer,kernel_regularizer = regularizer)(x)\n","  x = layers.BatchNormalization(epsilon=1.001e-5, name=name + '_2_bn')(x)\n","  x = layers.Activation('relu', name=name + '_2_relu')(x)\n","\n","  x = layers.Conv2D(4 * filters, 1, name=name + '_3_conv', kernel_initializer = initializer,kernel_regularizer = regularizer)(x)\n","  x = layers.Dropout(config[\"dropout\"])(x)\n","  x = layers.Add(name=name + '_out')([shortcut, x])\n","  return x\n","\n","def stack1(x, filters, blocks, stride1=2, name=None):\n","  \"\"\"A set of stacked residual blocks.\n","  Arguments:\n","    x: input tensor.\n","    filters: integer, filters of the bottleneck layer in a block.\n","    blocks: integer, blocks in the stacked blocks.\n","    stride1: default 2, stride of the first layer in the first block.\n","    name: string, stack label.\n","  Returns:\n","    Output tensor for the stacked blocks.\n","  \"\"\"\n","  x = block2(x, filters, conv_shortcut=True ,stride=stride1, name=name + '_block1')\n","  for i in range(2, blocks + 1):\n","    x = block2(x, filters, conv_shortcut=False, name=name + '_block' + str(i))\n","  return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ac1z7aIykj6l","colab_type":"code","colab":{}},"source":["def build_ResNet_Model(config = config_default):\n","  initializer = tf.keras.initializers.he_normal( seed = 3)\n","  alpha = config[\"weight_regu\"]  # weight decay coefficient\n","  regularizer = tf.keras.regularizers.l2(alpha)\n","  def stack_fn(x):\n","    x = stack1(x, 32, 2, stride1=1, name='conv2')\n","    x = stack1(x, 64, 2, name='conv3')\n","    x = stack1(x, 128, 2, name='conv4')\n","    return stack1(x, 256, 2, name='conv5')\n","  #input shape\n","  img_input = layers.Input(shape=(60,200,3))\n","\n","  \n","  x = layers.Conv2D(32, 3, strides=1,padding=\"same\", name='conv1_conv',kernel_initializer = initializer,kernel_regularizer = regularizer)(img_input)\n","  #pre act\n","  x = layers.MaxPooling2D(pool_size=(2,2), name='pool1_pool')(x)\n","  x = layers.Dropout(config[\"dropout\"])(x)\n","  x = stack_fn(x)\n","  \n","  x = layers.BatchNormalization(epsilon=1.001e-5, name='post_bn')(x)\n","  x = layers.Activation('relu', name='post_relu')(x)\n","  x = layers.GlobalAveragePooling2D()(x)\n","  #x = layers.MaxPooling2D(pool_size=(2,2))(x)\n","  #x = layers.Flatten()(x)\n","\n","  #classified\n","  digit1 = layers.Dense(36, name='digit1', activation='softmax', kernel_initializer = initializer,kernel_regularizer = regularizer)(x)\n","  digit2 = layers.Dense(36, name='digit2', activation='softmax', kernel_initializer = initializer,kernel_regularizer = regularizer)(x)\n","  digit3 = layers.Dense(36, name='digit3', activation='softmax', kernel_initializer = initializer,kernel_regularizer = regularizer)(x)\n","  digit4 = layers.Dense(36, name='digit4', activation='softmax', kernel_initializer = initializer,kernel_regularizer = regularizer)(x)\n","  digit5 = layers.Dense(36, name='digit5', activation='softmax', kernel_initializer = initializer,kernel_regularizer = regularizer)(x)\n","  digit6 = layers.Dense(36, name='digit6', activation='softmax', kernel_initializer = initializer,kernel_regularizer = regularizer)(x)\n","  modelt = tf.keras.models.Model(inputs=img_input,outputs = [digit1 , digit2, digit3, digit4, digit5, digit6], name = config[\"name\"])\n","  \n","\n","  \n","\n","  modelt.compile(\n","    loss=[\n","        tf.keras.losses.CategoricalCrossentropy(),\n","        tf.keras.losses.CategoricalCrossentropy(),\n","        tf.keras.losses.CategoricalCrossentropy(),\n","        tf.keras.losses.CategoricalCrossentropy(),\n","        tf.keras.losses.CategoricalCrossentropy(),\n","        tf.keras.losses.CategoricalCrossentropy(),\n","    ],\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=config[\"LR\"], beta_1=0.9), \n","    metrics=['accuracy'])\n","  return modelt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YVLUxBvFjN0h","colab_type":"code","colab":{}},"source":["  #railway model add 1 256 layer\n","def railway_model(config=config_default):\n","    initializer = tf.keras.initializers.he_normal( seed = 3)\n","    alpha = config[\"weight_regu\"]  # weight decay coefficient\n","    regularizer = tf.keras.regularizers.l2(alpha)\n","    dropout_rate =config[\"dropout\"]\n","    inn = Input((60, 200, 3))\n","    out = inn\n","    out = Conv2D(filters=36, kernel_size=(3, 3), padding='same', activation='relu',kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n","    out = Conv2D(filters=36, kernel_size=(3, 3), activation='relu',kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n","    out = BatchNormalization()(out)\n","    out = MaxPooling2D(pool_size=(2, 2))(out)\n","    out = Dropout(dropout_rate)(out)\n","    out = Conv2D(filters=72, kernel_size=(3, 3), padding='same', activation='relu',kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n","    out = Conv2D(filters=72, kernel_size=(3, 3), activation='relu',kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n","    out = BatchNormalization()(out)\n","    out = MaxPooling2D(pool_size=(2, 2))(out)\n","    out = Dropout(dropout_rate)(out)\n","    out = Conv2D(filters=144, kernel_size=(3, 3), padding='same', activation='relu',kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n","    out = Conv2D(filters=144, kernel_size=(3, 3), activation='relu',kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n","    out = BatchNormalization()(out)\n","    out = MaxPooling2D(pool_size=(2, 2))(out)\n","    out = Dropout(dropout_rate)(out)\n","    out = Conv2D(filters=288, kernel_size=(3, 3), padding='same',activation='relu',kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n","    out = Conv2D(filters=288, kernel_size=(3, 3), activation='relu',kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n","    out = BatchNormalization()(out)\n","    out = MaxPooling2D(pool_size=(2, 2))(out)\n","    out = Flatten()(out)\n","    sep = Dropout(dropout_rate)(out)\n","    dig1 = Dense(36, name='digit1', activation='softmax',kernel_initializer = initializer,kernel_regularizer = regularizer)(sep)\n","    dig2 = Dense(36, name='digit2', activation='softmax',kernel_initializer = initializer,kernel_regularizer = regularizer)(sep)\n","    dig3 = Dense(36, name='digit3', activation='softmax',kernel_initializer = initializer,kernel_regularizer = regularizer)(sep)\n","    dig4 = Dense(36, name='digit4', activation='softmax',kernel_initializer = initializer,kernel_regularizer = regularizer)(sep)\n","    dig5 = Dense(36, name='digit5', activation='softmax',kernel_initializer = initializer,kernel_regularizer = regularizer)(sep)\n","    dig6 = Dense(36, name='digit6', activation='softmax',kernel_initializer = initializer,kernel_regularizer = regularizer)(sep)\n","    model = tf.keras.models.Model(inputs=inn, outputs=[dig1, dig2,dig3,dig4,dig5,dig6],name=config[\"name\"])\n","    model.compile(\n","        loss=[\n","            tf.keras.losses.CategoricalCrossentropy(),\n","            tf.keras.losses.CategoricalCrossentropy(),\n","            tf.keras.losses.CategoricalCrossentropy(),\n","            tf.keras.losses.CategoricalCrossentropy(),\n","            tf.keras.losses.CategoricalCrossentropy(),\n","            tf.keras.losses.CategoricalCrossentropy(),\n","        ],\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=config[\"LR\"], beta_1=0.9),\n","        metrics=['accuracy'])\n","    model.summary()\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sq_8e_jVMwi8","colab_type":"text"},"source":["# Fit model"]},{"cell_type":"code","metadata":{"id":"9JWlf5zQ_P00","colab_type":"code","colab":{}},"source":["def trainModel(img_train, lb_train, img_test, lb_test,callB,epochs=100,config = config_default):\n","  history = model.fit(\n","        img_train,\n","        lb_train,\n","        batch_size = config[\"batch\"],\n","        shuffle = True,\n","        validation_data=(img_test, lb_test),\n","        epochs=epochs,\n","        verbose=1,\n","        callbacks = callB\n","        )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjYrIfL8P0G2","colab_type":"code","colab":{}},"source":["def switch_to_SGD():\n","  model.compile(\n","    loss=[\n","        tf.keras.losses.CategoricalCrossentropy(),\n","        tf.keras.losses.CategoricalCrossentropy(),\n","        tf.keras.losses.CategoricalCrossentropy(),\n","        tf.keras.losses.CategoricalCrossentropy(),\n","        tf.keras.losses.CategoricalCrossentropy(),\n","        tf.keras.losses.CategoricalCrossentropy()\n","    ],\n","    optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001, momentum=0.9), \n","    metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RhijEI2j7hrp","colab_type":"code","colab":{}},"source":["class myCallback(tf.keras.callbacks.Callback):\n","  def __init__(self, config=config_default,**kwargs):\n","    self.config = config\n","    self.best = 0\n","    self.epochs = 0\n","    self.wait = 0\n","    self.reduce_once = False\n","    self.patient = 8  #wait 8 epochs for early stopping\n","  def on_epoch_end(self, epoch, logs=None):\n","    self.epochs+=1\n","    b= np.prod([logs['digit1_accuracy'],\n","              logs['digit2_accuracy'],\n","              logs['digit3_accuracy'],\n","              logs['digit4_accuracy'],\n","              logs['digit5_accuracy'],\n","              logs['digit6_accuracy'],\n","              ])\n","    a= np.prod([logs['val_digit1_accuracy'],\n","              logs['val_digit2_accuracy'],\n","              logs['val_digit3_accuracy'],\n","              logs['val_digit4_accuracy'],\n","              logs['val_digit5_accuracy'],\n","              logs['val_digit6_accuracy'],\n","              ])\n","    wandb.log({\"loss\":logs[\"loss\"],\"val_loss\" : logs[\"val_loss\"],\"accuracy\": b, \"val_accuracy\":a, \"epoch\": self.epochs})\n","    print(\"\\nepoch: {}, loss: {:.4f}, val_loss: {:.4f}, accuracy: {:.5f}, val_accuracy: {:.5f}\".format(self.epochs,logs[\"loss\"],logs[\"val_loss\"],b,a))\n","    if a > self.best:\n","      print(\"                            ***** accuracy improved from {:.5f} to {:.5f}!! *****\".format(self.best,a))\n","      print(\"Model saved to \"+filepath+self.config[\"name\"]+\".hdf5\\n\" )\n","      self.best = a\n","      self.model.save(filepath+self.config[\"name\"]+\".hdf5\")\n","      self.wait = 0\n","      wandb.run.summary[\"best_val_accuracy\"] = a\n","      wandb.run.summary[\"best_epoch\"] = self.epochs\n","    else:\n","      if self.wait>=self.patient:   #switch to SGD or reduce lr if already on SGD \n","        if hasattr(self.model.optimizer,'momentum') and not self.reduce_once:     #reduce lr on SGD if not done it once yet\n","          self.model.optimizer.lr=self.model.optimizer.lr/2\n","          print(\"\\n\\n Change leraning rate to {}, continued ... \\n\".format(self.model.optimizer.lr))\n","          self.wait=0\n","          self.reduce_once=True\n","        else:\n","          self.model.stop_training = True\n","          self.wait=0\n","        print(\"Model is not learning, training stop at {}\".format(epoch))\n","      print(\"Model accuracy did not improve from {:.4f}\\n\".format(self.best))\n","      self.wait+=1\n","      \n","class Switch_SGD_callback(tf.keras.callbacks.Callback):\n","  def __init__(self,on_train_end, **kwargs):\n","    self.do_on_train_end=on_train_end\n","  def on_train_end(self, logs=None):\n","      self.do_on_train_end()\n","\n","def do_after_train():\n","    print(\"\\n ******** Switching to SGD!! **********\")\n","    switch_to_SGD()\n","    trainModel(img_train, lb_train, img_test, lb_test,[myCB])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YClSWCRVj9SY","colab_type":"code","colab":{}},"source":["model = build_ResNet_Model(config=myconfig)\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xx7TItLggDBf","colab_type":"code","colab":{}},"source":["#initialize callback object\n","myCB = myCallback(config=config)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["data 01目前最好的:\n","\n","val_accuracy: 98.11%\n","\n","dev01: 97.72%\n","\n","conv 3x3: 32,32,64,64,128,128,256,256\n","\n","weight_regu     0.005\n","\n","dropout         0.3\n","\n","batch           256\n","\n","備註: batch沒試過其他的，val和test有差0.4%可能是val不夠具有代表性，實際上存的不是最好的\n"]},{"cell_type":"code","metadata":{"id":"eBRXiFJSAqtj","colab_type":"code","colab":{}},"source":["#train for 100 epochs, switch to SGD if accuracy does not increase for 10 epochs, abort training it happens again\n","\n","myconfig = {\n","    \"stack\" : \"conv 3x3: 32   ,block2 : 32,64,128,256\",\n","    \"weight_regu\" : 1e-4,\n","    \"dropout\" : 0.3,\n","    \"preact\" : True,\n","    \"name\" : \"railway_ResNet_4\",\n","    \"output_WR\" : True,\n","    \"total_params\" : 3727704,\n","    \"opt_schedule\" : \"1e-3 adam + 1e-4 SGD\",\n","    \"batch\" : 64,\n","    \"GlobalAvg\":True,\n","    \"preact\":\"include shortcut\"\n","}\n","wandb.init(project=\"my-project-railway\",reinit=True,name=config[\"name\"],config=myconfig) \n","\n","#choose model\n","model = build_ResNet_Model(config=myconfig)\n","#ini call back\n","myCB = myCallback(config = myconfig)\n","trainModel(img_train, lb_train, img_test, lb_test,[myCB,Switch_SGD_callback(do_after_train)],config = myconfig)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_6980Q-GcKA","colab_type":"code","colab":{}},"source":["#manual switch to SGD\n","switch_to_SGD()\n","trainModel(img_train, lb_train, img_test, lb_test,[myCB])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Jg39Zk0QJj7","colab_type":"code","colab":{}},"source":["#Manual save\n","filepath=\"\"\n","model.save(filepath)"],"execution_count":0,"outputs":[]}],"metadata":{"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"AllCharacter_with_wandb.ipynb","provenance":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}