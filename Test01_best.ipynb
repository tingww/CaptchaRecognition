{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten, Dropout, MaxPooling2D,BatchNormalization\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = np.array([])\n",
    "for i in range(10):\n",
    "    CLASS_NAMES =np.append(CLASS_NAMES,chr(ord(\"0\")+i))\n",
    "for i in range(26):\n",
    "    CLASS_NAMES = np.append(CLASS_NAMES,chr(ord(\"A\")+i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "def parse_label(strIN):\n",
    "    return (strIN==CLASS_NAMES).astype(float)\n",
    "def readimg_to_tensor(fn):\n",
    "    a = tf.io.read_file((fn))\n",
    "    img = tf.io.decode_jpeg(a)\n",
    "    img = tf.image.convert_image_dtype(img, dtype=tf.float16)/255.\n",
    "    img = tf.image.per_image_standardization(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data 01\n",
    "def prepData01():  \n",
    "  train_dir_data01 = os.path.abspath(os.getcwd())+\"/train/data01_train/\"\n",
    "  data = pd.read_csv(\"./train/data01_train.csv\")\n",
    "  for i in range(len(data)):\n",
    "      data.iloc[i,1] = list(data.iloc[i,1])\n",
    "  arr = np.zeros([6,50000],str)\n",
    "  for j in range(6):\n",
    "      for i in range(len(data)):\n",
    "          arr[j][i] = data.iloc[i,1][j]\n",
    "  data = data.drop(columns= [\"code\"]).join(pd.DataFrame(arr.transpose(),columns = [\"code0\",\"code1\",'code2','code3','code4','code5']))\n",
    "\n",
    "  for i in range(6):\n",
    "      data.iloc[:,i+1] = data.iloc[:,i+1].map(parse_label)\n",
    "\n",
    "  TL = tf.convert_to_tensor(list(data.iloc[:,1].values))\n",
    "  TL2 = tf.convert_to_tensor(list(data.iloc[:,2].values))\n",
    "  TL3 = tf.convert_to_tensor(list(data.iloc[:,3].values))\n",
    "  TL4 = tf.convert_to_tensor(list(data.iloc[:,4].values))\n",
    "  TL5 = tf.convert_to_tensor(list(data.iloc[:,5].values))\n",
    "  TL6 = tf.convert_to_tensor(list(data.iloc[:,6].values))\n",
    "\n",
    "  train_dir = tf.data.Dataset.list_files(train_dir_data01+'*.jpg',shuffle=False) \n",
    "  train_data = train_dir.map(lambda x: readimg_to_tensor(x))\n",
    "  train_label = tf.data.Dataset.from_tensor_slices((TL,TL2,TL3,TL4,TL5,TL6))\n",
    "  train_data01=tf.data.Dataset.zip((train_data,train_label))\n",
    "  validation_split=0.1\n",
    "  num_elements=50000\n",
    "  split = int(num_elements*validation_split)\n",
    "  train_data_gen=iter(train_data01.shuffle(50000,seed = 102).batch(50000))\n",
    "\n",
    "  img , lb = next(train_data_gen)\n",
    "  img_train = img[:-split]\n",
    "  lb_train = (lb[0][:-split],lb[1][:-split],lb[2][:-split],lb[3][:-split],lb[4][:-split],lb[5][:-split])\n",
    "  img_test = img[-split:]\n",
    "  lb_test = (lb[0][-split:],lb[1][-split:],lb[2][-split:],lb[3][-split:],lb[4][-split:],lb[5][-split:])\n",
    "  return img_train, lb_train, img_test, lb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose data\n",
    "img_train, lb_train, img_test, lb_test= prepData01()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath =\"h5s/\" #\"/content/drive/Shared drives/TingWeis_Drive/Colab Notebooks/Saved Model/\"#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_default = {\n",
    "    \"weight_regu\":1e-6,\n",
    "    \"LR\":1e-3,\n",
    "    \"dropout\":0.3,\n",
    "    \"name\":\"Mymodel\",\n",
    "    \"batch\":256\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slyne config\n",
    "def Slyne_model(config=config_default):\n",
    "    initializer = tf.keras.initializers.he_normal( seed = 3)\n",
    "    alpha = config[\"weight_regu\"]  # weight decay coefficient\n",
    "    regularizer = tf.keras.regularizers.l2(alpha)\n",
    "    dropout_rate =config[\"dropout\"]\n",
    "    inn = Input((60, 200, 3))\n",
    "    out = inn\n",
    "    out = Conv2D(filters=64, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=64, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = Conv2D(filters=128, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=128, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = Conv2D(filters=256, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=256, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = Conv2D(filters=512, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=512, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = layers.GlobalAveragePooling2D()(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "    \n",
    "    out = layers.RepeatVector(6)(out)\n",
    "    sep = layers.GRU(128,return_sequences=True)(out)\n",
    "    sep0 = layers.Lambda(lambda x: x[:, 0, :])(sep)\n",
    "    sep1 = layers.Lambda(lambda x: x[:, 1, :])(sep)\n",
    "    sep2 = layers.Lambda(lambda x: x[:, 2, :])(sep)\n",
    "    sep3 = layers.Lambda(lambda x: x[:, 3, :])(sep)\n",
    "    sep4 = layers.Lambda(lambda x: x[:, 4, :])(sep)\n",
    "    sep5 = layers.Lambda(lambda x: x[:, 5, :])(sep)\n",
    "    dig1 = layers.Dense(36, name='digit1', activation='softmax')(sep0)\n",
    "    dig2 = layers.Dense(36, name='digit2', activation='softmax')(sep1)\n",
    "    dig3 = layers.Dense(36, name='digit3', activation='softmax')(sep2)\n",
    "    dig4 = layers.Dense(36, name='digit4', activation='softmax')(sep3)\n",
    "    dig5 = layers.Dense(36, name='digit5', activation='softmax')(sep4)\n",
    "    dig6 = layers.Dense(36, name='digit6', activation='softmax')(sep5)\n",
    "    model = tf.keras.models.Model(inputs=inn, outputs=[dig1, dig2,dig3,dig4,dig5,dig6],name=\"test\")\n",
    "    model.compile(\n",
    "            loss=[\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "            ],\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=myconfig[\"LR\"], beta_1=0.9),\n",
    "            metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(img_train, lb_train,callB,epochs=100,config = config_default):\n",
    "  history = model.fit(\n",
    "        img_train,\n",
    "        lb_train,\n",
    "        batch_size = config[\"batch\"],\n",
    "        shuffle = True,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks = callB\n",
    "        )\n",
    "def switch_to_SGD():\n",
    "  model.compile(\n",
    "    loss=[\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "        tf.keras.losses.CategoricalCrossentropy()\n",
    "    ],\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001, momentum=0.9), \n",
    "    metrics=['accuracy'])\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def __init__(self, config=config_default,**kwargs):\n",
    "    self.config = config\n",
    "    self.best = 0\n",
    "    self.epochs = 0\n",
    "    self.wait = 0\n",
    "    self.reduce_once = False\n",
    "    self.patient = 8  #wait 8 epochs for early stopping\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    self.epochs+=1\n",
    "    b= np.prod([logs['digit1_accuracy'],\n",
    "              logs['digit2_accuracy'],\n",
    "              logs['digit3_accuracy'],\n",
    "              logs['digit4_accuracy'],\n",
    "              logs['digit5_accuracy'],\n",
    "              logs['digit6_accuracy'],\n",
    "              ])\n",
    "    result = self.model.predict(img_test)\n",
    "    per_digit_acc,val_acc_all = allright(result,lb_test)\n",
    "    digit_acc = np.prod(per_digit_acc)\n",
    "    wandb.log({\"loss\":logs[\"loss\"],\"accuracy\": b, \"epoch\": self.epochs, \"val_accuracy_all\":val_acc_all, \"val_accuracy\":digit_acc})\n",
    "    print(\"\\nepoch: {}, loss: {:.4f}, accuracy: {:.5f}, val_accuracy_all: {:.5f}, val_accuracy: {:.5f}, val_per_digit_accuracy: {}\".format(\n",
    "        self.epochs,  logs[\"loss\"],   b,val_acc_all,    digit_acc,    per_digit_acc))\n",
    "    if val_acc_all> self.best:\n",
    "      print(\"                            ***** accuracy improved from {:.5f} to {:.5f}!! *****\".format(self.best,val_acc_all))\n",
    "      print(\"Model saved to \"+filepath+self.config[\"name\"]+\".hdf5\\n\" )\n",
    "      self.best = val_acc_all\n",
    "      self.model.save(filepath+self.config[\"name\"]+\".hdf5\")\n",
    "      self.wait = 0\n",
    "      wandb.run.summary[\"best_val_accuracy_all\"] = val_acc_all\n",
    "      wandb.run.summary[\"best_epoch\"] = self.epochs\n",
    "    elif self.epochs>30:\n",
    "      if self.wait>=self.patient:   #switch to SGD or reduce lr if already on SGD \n",
    "        if hasattr(self.model.optimizer,'momentum') and not self.reduce_once:     #reduce lr on SGD if not done it once yet\n",
    "          self.model.optimizer.lr=self.model.optimizer.lr/2\n",
    "          print(\"\\n\\n Change leraning rate to {}, continued ... \\n\".format(self.model.optimizer.lr))\n",
    "          self.wait=0\n",
    "          self.reduce_once=True\n",
    "        else:\n",
    "          self.model.stop_training = True\n",
    "          self.wait=0\n",
    "        print(\"Model is not learning, training stop at {}\".format(epoch))\n",
    "      print(\"Model accuracy did not improve from {:.4f}\\n\".format(self.best))\n",
    "      self.wait+=1\n",
    "      \n",
    "class Switch_SGD_callback(tf.keras.callbacks.Callback):\n",
    "  def __init__(self,on_train_end,config=config_default, **kwargs):\n",
    "    self.do_on_train_end=on_train_end\n",
    "    self.config = config\n",
    "  def on_train_end(self, logs=None):\n",
    "      self.do_on_train_end(self.config)\n",
    "\n",
    "def do_after_train(config=config_default):\n",
    "    '''do this after a seesion is over'''\n",
    "    print(\"\\n ******** Switching to SGD!! **********\")\n",
    "    switch_to_SGD()\n",
    "    trainModel(img_train, lb_train,[myCB],config=myconfig)\n",
    "\n",
    "def allright(inputdata,label):\n",
    "    '''calculate the accuracy of each digit and of all right'''\n",
    "    a = np.argmax(inputdata,axis = 2)\n",
    "    b = np.argmax(lb_test[0],axis=1)\n",
    "    b = np.expand_dims(b,0)\n",
    "    for i in range(5):\n",
    "        b = np.concatenate((b,np.expand_dims(np.argmax(lb_test[i+1],axis=1),0)),axis=0)\n",
    "    acc_digit = np.mean(a==b,axis=1)\n",
    "    acc_all = np.mean(np.all(a.transpose()==b.transpose(),axis=1))\n",
    "    return acc_digit,acc_all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myconfig = {\n",
    "    \"stack\" : \"conv 3x3 64-512 GRU 128 \",\n",
    "    \"weight_regu\" : 1e-5,\n",
    "    \"dropout\" : 0.2,\n",
    "    \"name\" : \"slyneGA_GRU2\",\n",
    "    \"output_WR\" : False,\n",
    "    \"total_params\" : 5066520,\n",
    "    \"opt_schedule\" : \"1e-4 adam + 1e-4 SGD\",\n",
    "    \"batch\" : 128,\n",
    "    \"GlobalAvg\":True,\n",
    "    \"Augmentation\":False,\n",
    "    \"LR\":1e-4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"my-project-railway\",reinit=True,name=myconfig[\"name\"],config=myconfig) \n",
    "#ini call back\n",
    "myCB = myCallback(config = myconfig)\n",
    "model=Slyne_model(config=myconfig)\n",
    "trainModel(img_train, lb_train,[myCB,Switch_SGD_callback(do_after_train,config = myconfig)],config = myconfig)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}