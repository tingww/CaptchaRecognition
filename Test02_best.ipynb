{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten, Dropout, MaxPooling2D,BatchNormalization\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = np.array([])\n",
    "for i in range(10):\n",
    "    CLASS_NAMES =np.append(CLASS_NAMES,chr(ord(\"0\")+i))\n",
    "for i in range(26):\n",
    "    CLASS_NAMES = np.append(CLASS_NAMES,chr(ord(\"A\")+i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no crop function\n",
    "def parse_label(strIN):\n",
    "    return (strIN==CLASS_NAMES).astype(float)\n",
    "def readimg_to_tensor(fn):\n",
    "    a = tf.io.read_file((fn))\n",
    "    img = tf.io.decode_jpeg(a)\n",
    "    img = tf.image.convert_image_dtype(img, dtype=tf.float16)/255.\n",
    "    img = tf.image.per_image_standardization(img)\n",
    "    return img\n",
    "def parseResult(RS):\n",
    "    for j in range(len(RS[0])):\n",
    "        PredChr=''\n",
    "        for i in range(6):\n",
    "            PredChr+= CLASS_NAMES[(np.where(RS[i][j] == RS[i][j].max())[0][0])]\n",
    "        if j == 0:\n",
    "            LB = [PredChr]\n",
    "        else:\n",
    "            LB.append(PredChr)      \n",
    "    return LB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data 02\n",
    "def prepData02(): \n",
    "  train_dir_data02 = os.path.abspath(os.getcwd())+\"/train/data02_train/\"\n",
    "  data2 = pd.read_csv(\"./train/data02_train.csv\")\n",
    "  for i in range(len(data2)):\n",
    "      data2.iloc[i,1] = list(data2.iloc[i,1])\n",
    "  arr = np.zeros([6,50000],str)\n",
    "  for j in range(6):\n",
    "      for i in range(len(data2)):\n",
    "          arr[j][i] = data2.iloc[i,1][j]\n",
    "  data2 = data2.drop(columns= [\"code\"]).join(pd.DataFrame(arr.transpose(),columns = [\"code0\",\"code1\",'code2','code3','code4','code5']))\n",
    "\n",
    "  #to one hot\n",
    "  for i in range(6):\n",
    "      data2.iloc[:,i+1] = data2.iloc[:,i+1].map(parse_label)\n",
    "\n",
    "  TL = tf.convert_to_tensor(list(data2.iloc[:,1].values))\n",
    "  TL2 = tf.convert_to_tensor(list(data2.iloc[:,2].values))\n",
    "  TL3 = tf.convert_to_tensor(list(data2.iloc[:,3].values))\n",
    "  TL4 = tf.convert_to_tensor(list(data2.iloc[:,4].values))\n",
    "  TL5 = tf.convert_to_tensor(list(data2.iloc[:,5].values))\n",
    "  TL6 = tf.convert_to_tensor(list(data2.iloc[:,6].values))\n",
    "\n",
    "  train_dir2 = tf.data.Dataset.list_files(train_dir_data02+'*.jpg',shuffle=False) \n",
    "  train_data2 = train_dir2.map(lambda x: readimg_to_tensor(x))\n",
    "  train_label2 = tf.data.Dataset.from_tensor_slices((TL,TL2,TL3,TL4,TL5,TL6))\n",
    "  train_data02=tf.data.Dataset.zip((train_data2,train_label2))\n",
    "  validation_split=0.1\n",
    "  num_elements=50000\n",
    "  split = int(num_elements*validation_split)\n",
    "  train_data_gen=iter(train_data02.shuffle(50000,seed = 1).batch(50000))\n",
    "\n",
    "  img , lb = next(train_data_gen)\n",
    "  img_train = img[:-split]\n",
    "  lb_train = (lb[0][:-split],lb[1][:-split],lb[2][:-split],lb[3][:-split],lb[4][:-split],lb[5][:-split])\n",
    "  img_test = img[-split:]\n",
    "  lb_test = (lb[0][-split:],lb[1][-split:],lb[2][-split:],lb[3][-split:],lb[4][-split:],lb[5][-split:])\n",
    "  return img_train, lb_train, img_test, lb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose data\n",
    "img_train, lb_train, img_test, lb_test= prepData02()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"h5s/Saved Model 02/\"\n",
    "config_default = {\n",
    "    \"weight_regu\":1e-6,\n",
    "    \"LR\":1e-3,\n",
    "    \"dropout\":0.3,\n",
    "    \"name\":\"Mymodel\",\n",
    "    \"batch\":256\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slyne config\n",
    "def Slyne_model(config=config_default):\n",
    "    initializer = tf.keras.initializers.he_normal( seed = 3)\n",
    "    alpha = config[\"weight_regu\"]  # weight decay coefficient\n",
    "    regularizer = tf.keras.regularizers.l2(alpha)\n",
    "    dropout_rate =config[\"dropout\"]\n",
    "    inn = Input((60, 200, 3))\n",
    "    out = inn\n",
    "    out = Conv2D(filters=64, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)#, kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=64, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)#, kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = Conv2D(filters=128, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)#, kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=128, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)#, kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = Conv2D(filters=256, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)#, kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=256, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)#, kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = Conv2D(filters=512, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)#, kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=512, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)#, kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = layers.GlobalAveragePooling2D()(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = layers.RepeatVector(6)(out)\n",
    "    #out = layers.GRU(128,return_sequences=True)(out)\n",
    "    out = layers.GRU(256,return_sequences=True,kernel_regularizer = regularizer)(out)\n",
    "    sep = Dropout(dropout_rate)(out)\n",
    "    sep0 = layers.Lambda(lambda x: x[:, 0, :])(sep)\n",
    "    sep1 = layers.Lambda(lambda x: x[:, 1, :])(sep)\n",
    "    sep2 = layers.Lambda(lambda x: x[:, 2, :])(sep)\n",
    "    sep3 = layers.Lambda(lambda x: x[:, 3, :])(sep)\n",
    "    sep4 = layers.Lambda(lambda x: x[:, 4, :])(sep)\n",
    "    sep5 = layers.Lambda(lambda x: x[:, 5, :])(sep)\n",
    "    dig1 = layers.Dense(36, name='digit1', activation='softmax', kernel_initializer = initializer)(sep0)\n",
    "    dig2 = layers.Dense(36, name='digit2', activation='softmax', kernel_initializer = initializer)(sep1)\n",
    "    dig3 = layers.Dense(36, name='digit3', activation='softmax', kernel_initializer = initializer)(sep2)\n",
    "    dig4 = layers.Dense(36, name='digit4', activation='softmax', kernel_initializer = initializer)(sep3)\n",
    "    dig5 = layers.Dense(36, name='digit5', activation='softmax', kernel_initializer = initializer)(sep4)\n",
    "    dig6 = layers.Dense(36, name='digit6', activation='softmax', kernel_initializer = initializer)(sep5)\n",
    "    model = tf.keras.models.Model(inputs=inn, outputs=[dig1, dig2,dig3,dig4,dig5,dig6],name=\"test\")\n",
    "    model.compile(\n",
    "            loss=[\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "            ],\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=myconfig[\"LR\"], beta_1=0.9),\n",
    "            metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(img_train, lb_train,callB,epochs=100,config = config_default):\n",
    "  history = model.fit(\n",
    "        img_train,\n",
    "        lb_train,\n",
    "        batch_size = config[\"batch\"],\n",
    "        shuffle = True,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks = callB\n",
    "        )\n",
    "def switch_to_SGD():\n",
    "  model.compile(\n",
    "    loss=[\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "        tf.keras.losses.CategoricalCrossentropy()\n",
    "    ],\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001, momentum=0.9), \n",
    "    metrics=['accuracy'])\n",
    "    \n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def __init__(self, config=config_default,**kwargs):\n",
    "    self.config = config\n",
    "    self.best = 0\n",
    "    self.epochs = 0\n",
    "    self.wait = 0\n",
    "    self.reduce_once = False\n",
    "    self.patient = 8  #wait 8 epochs for early stopping\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    self.epochs+=1\n",
    "    b= np.prod([logs['digit1_accuracy'],\n",
    "              logs['digit2_accuracy'],\n",
    "              logs['digit3_accuracy'],\n",
    "              logs['digit4_accuracy'],\n",
    "              logs['digit5_accuracy'],\n",
    "              logs['digit6_accuracy'],\n",
    "              ])\n",
    "    result = self.model.predict(img_test)\n",
    "    per_digit_acc,val_acc_all = allright(result,lb_test)\n",
    "    digit_acc = np.prod(per_digit_acc)\n",
    "    wandb.log({\"loss\":logs[\"loss\"],\"accuracy\": b, \"epoch\": self.epochs, \"val_accuracy_all\":val_acc_all, \"val_accuracy\":digit_acc})\n",
    "    print(\"\\nepoch: {}, loss: {:.4f}, accuracy: {:.5f}, val_accuracy_all: {:.5f}, val_accuracy: {:.5f}, val_per_digit_accuracy: {}\".format(\n",
    "        self.epochs,  logs[\"loss\"],   b,val_acc_all,    digit_acc,    per_digit_acc))\n",
    "    if val_acc_all> self.best:\n",
    "      print(\"                            ***** accuracy improved from {:.5f} to {:.5f}!! *****\".format(self.best,val_acc_all))\n",
    "      print(\"Model saved to \"+filepath+self.config[\"name\"]+\".hdf5\\n\" )\n",
    "      self.best = val_acc_all\n",
    "      self.model.save(filepath+self.config[\"name\"]+\".hdf5\")\n",
    "      self.wait = 0\n",
    "      wandb.run.summary[\"best_val_accuracy_all\"] = val_acc_all\n",
    "      wandb.run.summary[\"best_epoch\"] = self.epochs\n",
    "    else:\n",
    "      if self.wait>=self.patient:   #switch to SGD or reduce lr if already on SGD \n",
    "        if hasattr(self.model.optimizer,'momentum') and not self.reduce_once:     #reduce lr on SGD if not done it once yet\n",
    "          self.model.optimizer.lr=self.model.optimizer.lr/2\n",
    "          print(\"\\n\\n Change leraning rate to {}, continued ... \\n\".format(self.model.optimizer.lr))\n",
    "          self.wait=0\n",
    "          self.reduce_once=True\n",
    "        else:\n",
    "          self.model.stop_training = True\n",
    "          self.wait=0\n",
    "        print(\"Model is not learning, training stop at {}\".format(epoch))\n",
    "      print(\"Model accuracy did not improve from {:.4f}\\n\".format(self.best))\n",
    "      self.wait+=1\n",
    "      \n",
    "class Switch_SGD_callback(tf.keras.callbacks.Callback):\n",
    "  def __init__(self,on_train_end,config=config_default, **kwargs):\n",
    "    self.do_on_train_end=on_train_end\n",
    "    self.config = config\n",
    "  def on_train_end(self, logs=None):\n",
    "      self.do_on_train_end(self.config)\n",
    "\n",
    "def do_after_train(config=config_default):\n",
    "    '''do this after a seesion is over'''\n",
    "    print(\"\\n ******** Switching to SGD!! **********\")\n",
    "    switch_to_SGD()\n",
    "    trainModel(img_train, lb_train,[myCB],config=myconfig)\n",
    "\n",
    "def allright(inputdata,label):\n",
    "    '''calculate the accuracy of each digit and of all right'''\n",
    "    a = np.argmax(inputdata,axis = 2)\n",
    "    b = np.argmax(lb_test[0],axis=1)\n",
    "    b = np.expand_dims(b,0)\n",
    "    for i in range(5):\n",
    "        b = np.concatenate((b,np.expand_dims(np.argmax(lb_test[i+1],axis=1),0)),axis=0)\n",
    "    acc_digit = np.mean(a==b,axis=1)\n",
    "    acc_all = np.mean(np.all(a.transpose()==b.transpose(),axis=1))\n",
    "    return acc_digit,acc_all "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build models\n",
    "\n",
    "## Ensemble models:\n",
    "\n",
    "railway_d2e-1.hdf5\n",
    "\n",
    "CNN_GRU_2.hdf5\n",
    "\n",
    "CNN_GRU_3_fine2.hdf5\n",
    "\n",
    "CNN_GRU_4.hdf5\n",
    "\n",
    "CNN_GRU_5.hdf5\n",
    "\n",
    "CNN+GRU.hdf5\n",
    "\n",
    "\n",
    "\n",
    "val: 97.46%     test:97.39 %  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 0 - raiway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myconfig = {\n",
    "    \"stack\" : \"conv 3x3 32-256 \",\n",
    "    \"weight_regu\" : 0.005,\n",
    "    \"dropout\" : 0.2,\n",
    "    \"name\" : \"railway_d2e-1\",\n",
    "    \"output_WR\" : False,\n",
    "    \"total_params\" : 1727352,\n",
    "    \"opt_schedule\" : \"1e-3 adam + 1e-4 SGD\",\n",
    "    \"batch\" : 256,\n",
    "    \"LR\":1e-3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #railway model add 1 256 layer\n",
    "def railway_model(config=config_default):\n",
    "    initializer = tf.keras.initializers.he_normal( seed = 3)\n",
    "    alpha = config[\"weight_regu\"]  # weight decay coefficient\n",
    "    regularizer = tf.keras.regularizers.l2(alpha)\n",
    "    dropout_rate =config[\"dropout\"]\n",
    "    inn = Input((60, 200, 3))\n",
    "    out = inn\n",
    "    out = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu',kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = Conv2D(filters=32, kernel_size=(3, 3), activation='relu',kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "    out = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu',kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = Conv2D(filters=64, kernel_size=(3, 3), activation='relu',kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "    out = Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu',kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = Conv2D(filters=128, kernel_size=(3, 3), activation='relu',kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "    out = Conv2D(filters=256, kernel_size=(3, 3), padding='same',activation='relu',kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = Conv2D(filters=256, kernel_size=(3, 3), activation='relu',kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Flatten()(out)\n",
    "    sep = Dropout(dropout_rate)(out)\n",
    "    dig1 = Dense(36, name='digit1', activation='softmax',kernel_initializer = initializer,kernel_regularizer = regularizer)(sep)\n",
    "    dig2 = Dense(36, name='digit2', activation='softmax',kernel_initializer = initializer,kernel_regularizer = regularizer)(sep)\n",
    "    dig3 = Dense(36, name='digit3', activation='softmax',kernel_initializer = initializer,kernel_regularizer = regularizer)(sep)\n",
    "    dig4 = Dense(36, name='digit4', activation='softmax',kernel_initializer = initializer,kernel_regularizer = regularizer)(sep)\n",
    "    dig5 = Dense(36, name='digit5', activation='softmax',kernel_initializer = initializer,kernel_regularizer = regularizer)(sep)\n",
    "    dig6 = Dense(36, name='digit6', activation='softmax',kernel_initializer = initializer,kernel_regularizer = regularizer)(sep)\n",
    "    model = tf.keras.models.Model(inputs=inn, outputs=[dig1, dig2,dig3,dig4,dig5,dig6],name=config[\"name\"])\n",
    "    model.compile(\n",
    "        loss=[\n",
    "            tf.keras.losses.CategoricalCrossentropy(),\n",
    "            tf.keras.losses.CategoricalCrossentropy(),\n",
    "            tf.keras.losses.CategoricalCrossentropy(),\n",
    "            tf.keras.losses.CategoricalCrossentropy(),\n",
    "            tf.keras.losses.CategoricalCrossentropy(),\n",
    "            tf.keras.losses.CategoricalCrossentropy(),\n",
    "        ],\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=config[\"LR\"], beta_1=0.9),\n",
    "        metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"my-project-railway-data02\",reinit=True,name=myconfig[\"name\"],config=myconfig) \n",
    "#ini call back\n",
    "myCB = myCallback(config = myconfig)\n",
    "model = railway_model(config=myconfig)\n",
    "trainModel(img_train, lb_train,[myCB,Switch_SGD_callback(do_after_train,config = myconfig)],config = myconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 - CNN_GRU_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myconfig = {\n",
    "    \"stack\" : \"conv 3x3 64-512 GRU 1*128 \",\n",
    "    \"weight_regu\" : 1e-5,\n",
    "    \"dropout\" : 0.25,\n",
    "    \"name\" : \"CNN_GRU_2\",\n",
    "    \"output_WR\" : False,\n",
    "    \"total_params\" : 4967448,\n",
    "    \"opt_schedule\" : \"1e-3 adam + 1e-4 SGD\",\n",
    "    \"batch\" : 128,\n",
    "    \"GlobalAvg\":True,\n",
    "    \"Augmentation\":False,\n",
    "    \"LR\":1e-3\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(config=config_default):\n",
    "    initializer = tf.keras.initializers.he_normal( seed = 3)\n",
    "    alpha = config[\"weight_regu\"]  # weight decay coefficient\n",
    "    regularizer = tf.keras.regularizers.l2(alpha)\n",
    "    dropout_rate =config[\"dropout\"]\n",
    "    inn = Input((60, 200, 3))\n",
    "    out = inn\n",
    "    out = Conv2D(filters=64, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=64, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = Conv2D(filters=128, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=128, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = Conv2D(filters=256, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=256, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = Conv2D(filters=512, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=512, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = layers.GlobalAveragePooling2D()(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = layers.RepeatVector(6)(out)\n",
    "    out = layers.GRU(128,return_sequences=True,kernel_regularizer = regularizer)(out)\n",
    "    sep = Dropout(dropout_rate)(out)\n",
    "    sep0 = layers.Lambda(lambda x: x[:, 0, :])(sep)\n",
    "    sep1 = layers.Lambda(lambda x: x[:, 1, :])(sep)\n",
    "    sep2 = layers.Lambda(lambda x: x[:, 2, :])(sep)\n",
    "    sep3 = layers.Lambda(lambda x: x[:, 3, :])(sep)\n",
    "    sep4 = layers.Lambda(lambda x: x[:, 4, :])(sep)\n",
    "    sep5 = layers.Lambda(lambda x: x[:, 5, :])(sep)\n",
    "    dig1 = layers.Dense(36, name='digit1', activation='softmax', kernel_initializer = initializer)(sep0)\n",
    "    dig2 = layers.Dense(36, name='digit2', activation='softmax', kernel_initializer = initializer)(sep1)\n",
    "    dig3 = layers.Dense(36, name='digit3', activation='softmax', kernel_initializer = initializer)(sep2)\n",
    "    dig4 = layers.Dense(36, name='digit4', activation='softmax', kernel_initializer = initializer)(sep3)\n",
    "    dig5 = layers.Dense(36, name='digit5', activation='softmax', kernel_initializer = initializer)(sep4)\n",
    "    dig6 = layers.Dense(36, name='digit6', activation='softmax', kernel_initializer = initializer)(sep5)\n",
    "    model = tf.keras.models.Model(inputs=inn, outputs=[dig1, dig2,dig3,dig4,dig5,dig6],name=\"test\")\n",
    "    model.compile(\n",
    "            loss=[\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "            ],\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=myconfig[\"LR\"], beta_1=0.9),\n",
    "            metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"my-project-railway-data02\",reinit=True,name=myconfig[\"name\"],config=myconfig) \n",
    "#ini call back\n",
    "myCB = myCallback(config = myconfig)\n",
    "model = build_model(config=myconfig)\n",
    "trainModel(img_train, lb_train,[myCB,Switch_SGD_callback(do_after_train,config = myconfig)],config = myconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 - CNN_GRU_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myconfig = {\n",
    "    \"stack\" : \"conv 3x3 64-512 GRU 1*256 \",\n",
    "    \"weight_regu\" : 1e-5,\n",
    "    \"dropout\" : 0.25,\n",
    "    \"name\" : \"CNN_GRU_3\",\n",
    "    \"output_WR\" : False,\n",
    "    \"total_params\" : 5339928,\n",
    "    \"opt_schedule\" : \"1e-3 adam + 1e-4 SGD\",\n",
    "    \"batch\" : 128,\n",
    "    \"GlobalAvg\":True,\n",
    "    \"Augmentation\":False,\n",
    "    \"LR\":1e-3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(config=config_default):\n",
    "    initializer = tf.keras.initializers.he_normal( seed = 3)\n",
    "    alpha = config[\"weight_regu\"]  # weight decay coefficient\n",
    "    regularizer = tf.keras.regularizers.l2(alpha)\n",
    "    dropout_rate =config[\"dropout\"]\n",
    "    inn = Input((60, 200, 3))\n",
    "    out = inn\n",
    "    out = Conv2D(filters=64, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=64, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = Conv2D(filters=128, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=128, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = Conv2D(filters=256, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=256, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = Conv2D(filters=512, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=512, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = layers.GlobalAveragePooling2D()(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = layers.RepeatVector(6)(out)\n",
    "    out = layers.GRU(256,return_sequences=True,kernel_regularizer = regularizer)(out)\n",
    "    sep = Dropout(dropout_rate)(out)\n",
    "    sep0 = layers.Lambda(lambda x: x[:, 0, :])(sep)\n",
    "    sep1 = layers.Lambda(lambda x: x[:, 1, :])(sep)\n",
    "    sep2 = layers.Lambda(lambda x: x[:, 2, :])(sep)\n",
    "    sep3 = layers.Lambda(lambda x: x[:, 3, :])(sep)\n",
    "    sep4 = layers.Lambda(lambda x: x[:, 4, :])(sep)\n",
    "    sep5 = layers.Lambda(lambda x: x[:, 5, :])(sep)\n",
    "    dig1 = layers.Dense(36, name='digit1', activation='softmax', kernel_initializer = initializer)(sep0)\n",
    "    dig2 = layers.Dense(36, name='digit2', activation='softmax', kernel_initializer = initializer)(sep1)\n",
    "    dig3 = layers.Dense(36, name='digit3', activation='softmax', kernel_initializer = initializer)(sep2)\n",
    "    dig4 = layers.Dense(36, name='digit4', activation='softmax', kernel_initializer = initializer)(sep3)\n",
    "    dig5 = layers.Dense(36, name='digit5', activation='softmax', kernel_initializer = initializer)(sep4)\n",
    "    dig6 = layers.Dense(36, name='digit6', activation='softmax', kernel_initializer = initializer)(sep5)\n",
    "    model = tf.keras.models.Model(inputs=inn, outputs=[dig1, dig2,dig3,dig4,dig5,dig6],name=\"test\")\n",
    "    model.compile(\n",
    "            loss=[\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "            ],\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=myconfig[\"LR\"], beta_1=0.9),\n",
    "            metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"my-project-railway-data02\",reinit=True,name=myconfig[\"name\"],config=myconfig) \n",
    "#ini call back\n",
    "myCB = myCallback(config = myconfig)\n",
    "model = build_model(config=myconfig)\n",
    "trainModel(img_train, lb_train,[myCB,Switch_SGD_callback(do_after_train,config = myconfig)],config = myconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 - CNN_GRU_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myconfig = {\n",
    "    \"stack\" : \"conv 3x3 64-256 1024*2 GRU 1*256 \",\n",
    "    \"weight_regu\" : 1e-5,\n",
    "    \"dropout\" : 0.25,\n",
    "    \"name\" : \"CNN_GRU_4\",\n",
    "    \"output_WR\" : False,\n",
    "    \"total_params\" : 1399580,\n",
    "    \"opt_schedule\" : \"1e-3 adam + 1e-4 SGD\",\n",
    "    \"batch\" : 128,\n",
    "    \"GlobalAvg\":True,\n",
    "    \"Augmentation\":False,\n",
    "    \"LR\":1e-3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(config=config_default):\n",
    "    initializer = tf.keras.initializers.he_normal( seed = 3)\n",
    "    alpha = config[\"weight_regu\"]  # weight decay coefficient\n",
    "    regularizer = tf.keras.regularizers.l2(alpha)\n",
    "    dropout_rate =config[\"dropout\"]\n",
    "    inn = Input((60, 200, 3))\n",
    "    out = inn\n",
    "    out = Conv2D(filters=64, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=64, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = Conv2D(filters=128, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=128, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = Conv2D(filters=256, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=256, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = Conv2D(filters=1024, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=1024, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = layers.GlobalAveragePooling2D()(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = layers.RepeatVector(6)(out)\n",
    "    out = layers.GRU(256,return_sequences=True,kernel_regularizer = regularizer)(out)\n",
    "    sep = Dropout(dropout_rate)(out)\n",
    "    sep0 = layers.Lambda(lambda x: x[:, 0, :])(sep)\n",
    "    sep1 = layers.Lambda(lambda x: x[:, 1, :])(sep)\n",
    "    sep2 = layers.Lambda(lambda x: x[:, 2, :])(sep)\n",
    "    sep3 = layers.Lambda(lambda x: x[:, 3, :])(sep)\n",
    "    sep4 = layers.Lambda(lambda x: x[:, 4, :])(sep)\n",
    "    sep5 = layers.Lambda(lambda x: x[:, 5, :])(sep)\n",
    "    dig1 = layers.Dense(36, name='digit1', activation='softmax', kernel_initializer = initializer)(sep0)\n",
    "    dig2 = layers.Dense(36, name='digit2', activation='softmax', kernel_initializer = initializer)(sep1)\n",
    "    dig3 = layers.Dense(36, name='digit3', activation='softmax', kernel_initializer = initializer)(sep2)\n",
    "    dig4 = layers.Dense(36, name='digit4', activation='softmax', kernel_initializer = initializer)(sep3)\n",
    "    dig5 = layers.Dense(36, name='digit5', activation='softmax', kernel_initializer = initializer)(sep4)\n",
    "    dig6 = layers.Dense(36, name='digit6', activation='softmax', kernel_initializer = initializer)(sep5)\n",
    "    model = tf.keras.models.Model(inputs=inn, outputs=[dig1, dig2,dig3,dig4,dig5,dig6],name=\"test\")\n",
    "    model.compile(\n",
    "            loss=[\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "            ],\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=myconfig[\"LR\"], beta_1=0.9),\n",
    "            metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"my-project-railway-data02\",reinit=True,name=myconfig[\"name\"],config=myconfig) \n",
    "#ini call back\n",
    "myCB = myCallback(config = myconfig)\n",
    "model = build_model(config=myconfig)\n",
    "trainModel(img_train, lb_train,[myCB,Switch_SGD_callback(do_after_train,config = myconfig)],config = myconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4 - CNN_GRU_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myconfig = {\n",
    "    \"stack\" : \"conv 3x3 64-256 512*2*6 GRU 1*128 \",\n",
    "    \"weight_regu\" : 1e-6,\n",
    "    \"dropout\" : 0.25,\n",
    "    \"name\" : \"CNN_GRU_5\",\n",
    "    \"output_WR\" : False,\n",
    "    \"total_params\" : 23060248,\n",
    "    \"opt_schedule\" : \"1e-3 adam + 1e-4 SGD\",\n",
    "    \"batch\" : 128,\n",
    "    \"GlobalAvg\":True,\n",
    "    \"Augmentation\":False,\n",
    "    \"LR\":1e-3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myblock(out,config=myconfig):\n",
    "    initializer = tf.keras.initializers.he_normal( seed = 3)\n",
    "    alpha = config[\"weight_regu\"]  # weight decay coefficient\n",
    "    regularizer = tf.keras.regularizers.l2(alpha)\n",
    "    dropout_rate =config[\"dropout\"]\n",
    "    out = Conv2D(filters=512, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=512, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = layers.GlobalAveragePooling2D()(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "    out = layers.Reshape((1,512))(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(config=config_default):\n",
    "    initializer = tf.keras.initializers.he_normal( seed = 3)\n",
    "    alpha = config[\"weight_regu\"]  # weight decay coefficient\n",
    "    regularizer = tf.keras.regularizers.l2(alpha)\n",
    "    dropout_rate =config[\"dropout\"]\n",
    "    inn = Input((60, 200, 3))\n",
    "    out = inn\n",
    "    out = Conv2D(filters=64, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=64, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = Conv2D(filters=128, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=128, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = Conv2D(filters=256, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=256, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out1 = myblock(out)\n",
    "    out2 = myblock(out)\n",
    "    out3 = myblock(out)\n",
    "    out4 = myblock(out)\n",
    "    out5 = myblock(out)\n",
    "    out6 = myblock(out)\n",
    "\n",
    "    out = layers.concatenate((out1,out2,out3,out4,out5,out6),axis=1)\n",
    "    out = layers.GRU(256,return_sequences=True,kernel_regularizer = regularizer)(out)\n",
    "    sep = Dropout(dropout_rate)(out)\n",
    "    sep0 = layers.Lambda(lambda x: x[:, 0, :])(sep)\n",
    "    sep1 = layers.Lambda(lambda x: x[:, 1, :])(sep)\n",
    "    sep2 = layers.Lambda(lambda x: x[:, 2, :])(sep)\n",
    "    sep3 = layers.Lambda(lambda x: x[:, 3, :])(sep)\n",
    "    sep4 = layers.Lambda(lambda x: x[:, 4, :])(sep)\n",
    "    sep5 = layers.Lambda(lambda x: x[:, 5, :])(sep)\n",
    "    dig1 = layers.Dense(36, name='digit1', activation='softmax', kernel_initializer = initializer)(sep0)\n",
    "    dig2 = layers.Dense(36, name='digit2', activation='softmax', kernel_initializer = initializer)(sep1)\n",
    "    dig3 = layers.Dense(36, name='digit3', activation='softmax', kernel_initializer = initializer)(sep2)\n",
    "    dig4 = layers.Dense(36, name='digit4', activation='softmax', kernel_initializer = initializer)(sep3)\n",
    "    dig5 = layers.Dense(36, name='digit5', activation='softmax', kernel_initializer = initializer)(sep4)\n",
    "    dig6 = layers.Dense(36, name='digit6', activation='softmax', kernel_initializer = initializer)(sep5)\n",
    "    model = tf.keras.models.Model(inputs=inn, outputs=[dig1, dig2,dig3,dig4,dig5,dig6],name=\"test\")\n",
    "    model.compile(\n",
    "            loss=[\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "            ],\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=myconfig[\"LR\"], beta_1=0.9),\n",
    "            metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"my-project-railway-data02\",reinit=True,name=myconfig[\"name\"],config=myconfig) \n",
    "#ini call back\n",
    "myCB = myCallback(config = myconfig)\n",
    "model = build_model(config=myconfig)\n",
    "trainModel(img_train, lb_train,[myCB,Switch_SGD_callback(do_after_train,config = myconfig)],config = myconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5 - CNN+GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myconfig = {\n",
    "    \"stack\" : \"conv 3x3 64-512 GRU 2*128 \",\n",
    "    \"weight_regu\" : 1e-5,\n",
    "    \"dropout\" : 0.25,\n",
    "    \"name\" : \"CNN+GRU\",\n",
    "    \"output_WR\" : False,\n",
    "    \"total_params\" : 5066520,\n",
    "    \"opt_schedule\" : \"1e-3 adam + 1e-4 SGD\",\n",
    "    \"batch\" : 128,\n",
    "    \"GlobalAvg\":True,\n",
    "    \"Augmentation\":False,\n",
    "    \"LR\":1e-3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(config=config_default):\n",
    "    initializer = tf.keras.initializers.he_normal( seed = 3)\n",
    "    alpha = config[\"weight_regu\"]  # weight decay coefficient\n",
    "    regularizer = tf.keras.regularizers.l2(alpha)\n",
    "    dropout_rate =config[\"dropout\"]\n",
    "    inn = Input((60, 200, 3))\n",
    "    out = inn\n",
    "    out = Conv2D(filters=64, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=64, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = Conv2D(filters=128, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=128, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = Conv2D(filters=256, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=256, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = Conv2D(filters=512, kernel_size=(3, 3), padding='valid', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = Conv2D(filters=512, kernel_size=(3, 3), padding='same', kernel_initializer = initializer,kernel_regularizer = regularizer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = layers.GlobalAveragePooling2D()(out)\n",
    "    out = Dropout(dropout_rate)(out)\n",
    "\n",
    "    out = layers.RepeatVector(6)(out)\n",
    "    out = layers.GRU(128,return_sequences=True,kernel_regularizer = regularizer)(out)\n",
    "    out = layers.GRU(128,return_sequences=True,kernel_regularizer = regularizer)(out)\n",
    "    sep = Dropout(dropout_rate)(out)\n",
    "    sep0 = layers.Lambda(lambda x: x[:, 0, :])(sep)\n",
    "    sep1 = layers.Lambda(lambda x: x[:, 1, :])(sep)\n",
    "    sep2 = layers.Lambda(lambda x: x[:, 2, :])(sep)\n",
    "    sep3 = layers.Lambda(lambda x: x[:, 3, :])(sep)\n",
    "    sep4 = layers.Lambda(lambda x: x[:, 4, :])(sep)\n",
    "    sep5 = layers.Lambda(lambda x: x[:, 5, :])(sep)\n",
    "    dig1 = layers.Dense(36, name='digit1', activation='softmax', kernel_initializer = initializer)(sep0)\n",
    "    dig2 = layers.Dense(36, name='digit2', activation='softmax', kernel_initializer = initializer)(sep1)\n",
    "    dig3 = layers.Dense(36, name='digit3', activation='softmax', kernel_initializer = initializer)(sep2)\n",
    "    dig4 = layers.Dense(36, name='digit4', activation='softmax', kernel_initializer = initializer)(sep3)\n",
    "    dig5 = layers.Dense(36, name='digit5', activation='softmax', kernel_initializer = initializer)(sep4)\n",
    "    dig6 = layers.Dense(36, name='digit6', activation='softmax', kernel_initializer = initializer)(sep5)\n",
    "    model = tf.keras.models.Model(inputs=inn, outputs=[dig1, dig2,dig3,dig4,dig5,dig6],name=\"test\")\n",
    "    model.compile(\n",
    "            loss=[\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                tf.keras.losses.CategoricalCrossentropy(),\n",
    "            ],\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=myconfig[\"LR\"], beta_1=0.9),\n",
    "            metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"my-project-railway-data02\",reinit=True,name=myconfig[\"name\"],config=myconfig) \n",
    "#ini call back\n",
    "myCB = myCallback(config = myconfig)\n",
    "model = build_model(config=myconfig)\n",
    "trainModel(img_train, lb_train,[myCB,Switch_SGD_callback(do_after_train,config = myconfig)],config = myconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"h5s/Saved Model 02/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths=[\"railway_d2e-1.hdf5\",\"CNN_GRU_3_fine2.hdf5\",\"CNN_GRU_4.hdf5\",\"CNN_GRU_5.hdf5\",\"CNN+GRU.hdf5\",\"CNN_GRU_2.hdf5\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_chr(model_paths,img_test):\n",
    "    '''return the most probable characters for given models'''\n",
    "    models = []\n",
    "    for m in model_paths:\n",
    "        models.append(tf.keras.models.load_model(filepath+m))\n",
    "    result = []\n",
    "    for m in models:\n",
    "        result.append(m.predict(img_test))\n",
    "    r = np.array(result)\n",
    "    a = np.argmax(r,axis=3)\n",
    "    a = a.transpose()\n",
    "    a_p = np.max(r,axis=3)\n",
    "    a_p = a_p.transpose()\n",
    "    #return the most probable character index\n",
    "    vote_outcome=[]\n",
    "    for j in range(len(a)):\n",
    "        chr = ''\n",
    "        for i,p in zip(a[j],a_p[j]):\n",
    "            if(all(i==i[0])):\n",
    "                chr+=(CLASS_NAMES[i[0]])\n",
    "            else:\n",
    "                maj = np.bincount(i).argmax()   #i[np.argmax(p)]\n",
    "                chr+=(CLASS_NAMES[maj])           \n",
    "        vote_outcome.append(chr)\n",
    "    \n",
    "    return vote_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#02\n",
    "val_dir_data02 = os.path.abspath(os.getcwd())+\"\\\\test\\\\data02_test\"\n",
    "#validation data no crop\n",
    "val_dir = tf.data.Dataset.list_files(val_dir_data02+'*.jpg',shuffle=False) \n",
    "val_data02 = val_dir.map(lambda x: readimg_to_tensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output to CSV\n",
    "img = next(iter(val_data02.batch(10000)))\n",
    "test = ensemble_chr(model_paths,img)\n",
    "val_file_name = pd.read_csv(\"./dev/data02_dev.csv\")\n",
    "val_file_name.join(pd.DataFrame(test,columns=[\"code\"])).to_csv(\"data02_test.csv\",index = False) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}