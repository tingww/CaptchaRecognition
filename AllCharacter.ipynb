{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train data count: 50000\nValidation data count: 10000\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         filename code0 code1 code2 code3 code4 code5\n0      000000.jpg     Z     3     2     7     0     D\n1      000001.jpg     V     M     Y     X     E     8\n2      000002.jpg     J     Z     K     W     V     U\n3      000003.jpg     X     9     I     2     7     H\n4      000004.jpg     H     5     X     G     R     2\n...           ...   ...   ...   ...   ...   ...   ...\n49995  049995.jpg     3     Y     F     I     S     E\n49996  049996.jpg     4     S     U     T     F     7\n49997  049997.jpg     C     Y     U     N     0     9\n49998  049998.jpg     K     C     D     8     I     O\n49999  049999.jpg     8     X     L     C     2     V\n\n[50000 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>code0</th>\n      <th>code1</th>\n      <th>code2</th>\n      <th>code3</th>\n      <th>code4</th>\n      <th>code5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000000.jpg</td>\n      <td>Z</td>\n      <td>3</td>\n      <td>2</td>\n      <td>7</td>\n      <td>0</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000001.jpg</td>\n      <td>V</td>\n      <td>M</td>\n      <td>Y</td>\n      <td>X</td>\n      <td>E</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000002.jpg</td>\n      <td>J</td>\n      <td>Z</td>\n      <td>K</td>\n      <td>W</td>\n      <td>V</td>\n      <td>U</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000003.jpg</td>\n      <td>X</td>\n      <td>9</td>\n      <td>I</td>\n      <td>2</td>\n      <td>7</td>\n      <td>H</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>000004.jpg</td>\n      <td>H</td>\n      <td>5</td>\n      <td>X</td>\n      <td>G</td>\n      <td>R</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49995</th>\n      <td>049995.jpg</td>\n      <td>3</td>\n      <td>Y</td>\n      <td>F</td>\n      <td>I</td>\n      <td>S</td>\n      <td>E</td>\n    </tr>\n    <tr>\n      <th>49996</th>\n      <td>049996.jpg</td>\n      <td>4</td>\n      <td>S</td>\n      <td>U</td>\n      <td>T</td>\n      <td>F</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>049997.jpg</td>\n      <td>C</td>\n      <td>Y</td>\n      <td>U</td>\n      <td>N</td>\n      <td>0</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>049998.jpg</td>\n      <td>K</td>\n      <td>C</td>\n      <td>D</td>\n      <td>8</td>\n      <td>I</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>049999.jpg</td>\n      <td>8</td>\n      <td>X</td>\n      <td>L</td>\n      <td>C</td>\n      <td>2</td>\n      <td>V</td>\n    </tr>\n  </tbody>\n</table>\n<p>50000 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten, Dropout, MaxPooling2D,BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_size = 300000\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "IMG_HEIGHT = 60\n",
    "IMG_WIDTH = 28\n",
    "\n",
    "CLASS_NAMES = np.array([])\n",
    "for i in range(10):\n",
    "    CLASS_NAMES =np.append(CLASS_NAMES,chr(ord(\"0\")+i))\n",
    "for i in range(26):\n",
    "    CLASS_NAMES = np.append(CLASS_NAMES,chr(ord(\"A\")+i))\n",
    "\n",
    "#training dir / validation dir\n",
    "train_dir_data01 = os.path.abspath(os.getcwd())+\"\\\\train\\\\data01_train\\\\\"\n",
    "val_dir_data01 = os.path.abspath(os.getcwd())+\"\\\\dev\\\\data01_dev\"\n",
    "num_train_data01 = len(os.listdir(train_dir_data01))\n",
    "num_val_data01 = len(os.listdir(val_dir_data01))\n",
    "print(\"Train data count: \" + str(num_train_data01))\n",
    "print(\"Validation data count: \" + str(num_val_data01))\n",
    "\n",
    "#store filename and label to a Dataframe \n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"train\\\\data01_train.csv\")\n",
    "for i in range(len(data)):\n",
    "    data.iloc[i,1] = list(data.iloc[i,1])\n",
    "\n",
    "#maybe there's an easier way to separate labels??\n",
    "arr = np.zeros([6,50000],str)\n",
    "for j in range(6):\n",
    "    for i in range(len(data)):\n",
    "        arr[j][i] = data.iloc[i,1][j]\n",
    "data = data.drop(columns= [\"code\"]).join(pd.DataFrame(arr.transpose(),columns = [\"code0\",\"code1\",'code2','code3','code4','code5']))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data 02\n",
    "train_dir_data02 = os.path.abspath(os.getcwd())+\"\\\\train\\\\data02_train\\\\\"\n",
    "data2 = pd.read_csv(\"train\\\\data02_train.csv\")\n",
    "for i in range(len(data2)):\n",
    "    data2.iloc[i,1] = list(data2.iloc[i,1])\n",
    "arr = np.zeros([6,50000],str)\n",
    "for j in range(6):\n",
    "    for i in range(len(data2)):\n",
    "        arr[j][i] = data2.iloc[i,1][j]\n",
    "data2 = data2.drop(columns= [\"code\"]).join(pd.DataFrame(arr.transpose(),columns = [\"code0\",\"code1\",'code2','code3','code4','code5']))\n",
    "\n",
    "#to one hot\n",
    "for i in range(6):\n",
    "    data2.iloc[:,i+1] = data2.iloc[:,i+1].map(parse_label)\n",
    "\n",
    "TL = tf.convert_to_tensor(list(data2.iloc[:,1].values))\n",
    "TL2 = tf.convert_to_tensor(list(data2.iloc[:,2].values))\n",
    "TL3 = tf.convert_to_tensor(list(data2.iloc[:,3].values))\n",
    "TL4 = tf.convert_to_tensor(list(data2.iloc[:,4].values))\n",
    "TL5 = tf.convert_to_tensor(list(data2.iloc[:,5].values))\n",
    "TL6 = tf.convert_to_tensor(list(data2.iloc[:,6].values))\n",
    "\n",
    "train_dir2 = tf.data.Dataset.list_files(train_dir_data02+'*.jpg',shuffle=False) \n",
    "train_data2 = train_dir2.map(lambda x: readimg_to_tensor(x))\n",
    "train_label2 = tf.data.Dataset.from_tensor_slices((TL,TL2,TL3,TL4,TL5,TL6))\n",
    "train_data02=tf.data.Dataset.zip((train_data2,train_label2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no crop function\n",
    "def parse_label(strIN):\n",
    "    return (strIN==CLASS_NAMES).astype(float)\n",
    "def readimg_to_tensor(fn):\n",
    "    a = tf.io.read_file((fn))\n",
    "        #crop to [60,30,3]\n",
    "    img = tf.io.decode_jpeg(a)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         filename                                              code0  \\\n0      000000.jpg  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n1      000001.jpg  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n2      000002.jpg  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n3      000003.jpg  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n4      000004.jpg  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n...           ...                                                ...   \n49995  049995.jpg  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n49996  049996.jpg  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...   \n49997  049997.jpg  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n49998  049998.jpg  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n49999  049999.jpg  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...   \n\n                                                   code1  \\\n0      [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n1      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n2      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n3      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n4      [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...   \n...                                                  ...   \n49995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n49996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n49997  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n49998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n49999  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n\n                                                   code2  \\\n0      [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n1      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n2      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n3      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n4      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n...                                                  ...   \n49995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n49996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n49997  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n49998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n49999  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n\n                                                   code3  \\\n0      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...   \n1      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n2      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n3      [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n4      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n...                                                  ...   \n49995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n49996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n49997  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n49998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...   \n49999  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n\n                                                   code4  \\\n0      [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n1      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n2      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n3      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...   \n4      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n...                                                  ...   \n49995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n49996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n49997  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n49998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n49999  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n\n                                                   code5  \n0      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n1      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n2      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n3      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n4      [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n...                                                  ...  \n49995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n49996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n49997  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n49998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n49999  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n\n[50000 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>code0</th>\n      <th>code1</th>\n      <th>code2</th>\n      <th>code3</th>\n      <th>code4</th>\n      <th>code5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000000.jpg</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000001.jpg</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000002.jpg</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000003.jpg</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>000004.jpg</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49995</th>\n      <td>049995.jpg</td>\n      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>49996</th>\n      <td>049996.jpg</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>049997.jpg</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>049998.jpg</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>049999.jpg</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>50000 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "#to one hot\n",
    "for i in range(6):\n",
    "    data.iloc[:,i+1] = data.iloc[:,i+1].map(parse_label)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TL = tf.convert_to_tensor(list(data.iloc[:,1].values))\n",
    "TL2 = tf.convert_to_tensor(list(data.iloc[:,2].values))\n",
    "TL3 = tf.convert_to_tensor(list(data.iloc[:,3].values))\n",
    "TL4 = tf.convert_to_tensor(list(data.iloc[:,4].values))\n",
    "TL5 = tf.convert_to_tensor(list(data.iloc[:,5].values))\n",
    "TL6 = tf.convert_to_tensor(list(data.iloc[:,6].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no crop\n",
    "train_dir = tf.data.Dataset.list_files(train_dir_data01+'*.jpg',shuffle=False) \n",
    "train_data = train_dir.map(lambda x: readimg_to_tensor(x))\n",
    "train_label = tf.data.Dataset.from_tensor_slices((TL,TL2,TL3,TL4,TL5,TL6))\n",
    "train_data01=tf.data.Dataset.zip((train_data,train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation data no crop\n",
    "val_dir = tf.data.Dataset.list_files(val_dir_data01+'*.jpg',shuffle=False) \n",
    "val_data01 = val_dir.map(lambda x: readimg_to_tensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Creating CNN model...\nModel: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 60, 200, 3)] 0                                            \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 60, 200, 32)  896         input_1[0][0]                    \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 58, 198, 32)  9248        conv2d[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 58, 198, 32)  128         conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 29, 99, 32)   0           batch_normalization[0][0]        \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 29, 99, 32)   0           max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 29, 99, 64)   18496       dropout[0][0]                    \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 27, 97, 64)   36928       conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 27, 97, 64)   256         conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 13, 48, 64)   0           batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 13, 48, 64)   0           max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 13, 48, 128)  73856       dropout_1[0][0]                  \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 11, 46, 128)  147584      conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_2 (BatchNor (None, 11, 46, 128)  512         conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 5, 23, 128)   0           batch_normalization_2[0][0]      \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 5, 23, 128)   0           max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 3, 21, 256)   295168      dropout_2[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_3 (BatchNor (None, 3, 21, 256)   1024        conv2d_6[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 1, 10, 256)   0           batch_normalization_3[0][0]      \n__________________________________________________________________________________________________\nflatten (Flatten)               (None, 2560)         0           max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 2560)         0           flatten[0][0]                    \n__________________________________________________________________________________________________\ndigit1 (Dense)                  (None, 36)           92196       dropout_3[0][0]                  \n__________________________________________________________________________________________________\ndigit2 (Dense)                  (None, 36)           92196       dropout_3[0][0]                  \n__________________________________________________________________________________________________\ndigit3 (Dense)                  (None, 36)           92196       dropout_3[0][0]                  \n__________________________________________________________________________________________________\ndigit4 (Dense)                  (None, 36)           92196       dropout_3[0][0]                  \n__________________________________________________________________________________________________\ndigit5 (Dense)                  (None, 36)           92196       dropout_3[0][0]                  \n__________________________________________________________________________________________________\ndigit6 (Dense)                  (None, 36)           92196       dropout_3[0][0]                  \n==================================================================================================\nTotal params: 1,137,272\nTrainable params: 1,136,312\nNon-trainable params: 960\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "#railway model\n",
    "print(\"Creating CNN model...\")\n",
    "inn = Input((60, 200, 3))\n",
    "out = inn\n",
    "out = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu')(out)\n",
    "out = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "out = Dropout(0.3)(out)\n",
    "out = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(out)\n",
    "out = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "out = Dropout(0.3)(out)\n",
    "out = Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu')(out)\n",
    "out = Conv2D(filters=128, kernel_size=(3, 3), activation='relu')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "out = Dropout(0.3)(out)\n",
    "out = Conv2D(filters=256, kernel_size=(3, 3), activation='relu')(out)\n",
    "out = BatchNormalization()(out)\n",
    "out = MaxPooling2D(pool_size=(2, 2))(out)\n",
    "out = Flatten()(out)\n",
    "sep = Dropout(0.3)(out)\n",
    "dig1 = Dense(36, name='digit1', activation='softmax')(sep)\n",
    "dig2 = Dense(36, name='digit2', activation='softmax')(sep)\n",
    "dig3 = Dense(36, name='digit3', activation='softmax')(sep)\n",
    "dig4 = Dense(36, name='digit4', activation='softmax')(sep)\n",
    "dig5 = Dense(36, name='digit5', activation='softmax')(sep)\n",
    "dig6 = Dense(36, name='digit6', activation='softmax')(sep)\n",
    "model = tf.keras.models.Model(inputs=inn, outputs=[dig1, dig2,dig3,dig4,dig5,dig6])\n",
    "model.compile(\n",
    "    loss=[\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "    ],\n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "50000"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "#number of element\n",
    "num_elements = tf.data.experimental.cardinality(train_data01).numpy()\n",
    "num_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "s: 0.0882 - val_digit4_loss: 0.1743 - val_digit5_loss: 0.1875 - val_digit6_loss: 0.1896 - val_digit1_accuracy: 0.9540 - val_digit2_accuracy: 0.9360 - val_digit3_accuracy: 0.9800 - val_digit4_accuracy: 0.9620 - val_digit5_accuracy: 0.9420 - val_digit6_accuracy: 0.9340\nEpoch 2/15\n36/36 - 7s - loss: 0.9585 - digit1_loss: 0.1543 - digit2_loss: 0.1552 - digit3_loss: 0.1359 - digit4_loss: 0.1658 - digit5_loss: 0.1796 - digit6_loss: 0.1676 - digit1_accuracy: 0.9511 - digit2_accuracy: 0.9531 - digit3_accuracy: 0.9544 - digit4_accuracy: 0.9480 - digit5_accuracy: 0.9442 - digit6_accuracy: 0.9484 - val_loss: 1.3049 - val_digit1_loss: 0.1919 - val_digit2_loss: 0.2241 - val_digit3_loss: 0.1658 - val_digit4_loss: 0.2181 - val_digit5_loss: 0.2534 - val_digit6_loss: 0.2516 - val_digit1_accuracy: 0.9420 - val_digit2_accuracy: 0.9320 - val_digit3_accuracy: 0.9580 - val_digit4_accuracy: 0.9400 - val_digit5_accuracy: 0.9380 - val_digit6_accuracy: 0.9320\nEpoch 3/15\n36/36 - 7s - loss: 0.6716 - digit1_loss: 0.1116 - digit2_loss: 0.0946 - digit3_loss: 0.0988 - digit4_loss: 0.1164 - digit5_loss: 0.1196 - digit6_loss: 0.1307 - digit1_accuracy: 0.9660 - digit2_accuracy: 0.9691 - digit3_accuracy: 0.9676 - digit4_accuracy: 0.9587 - digit5_accuracy: 0.9640 - digit6_accuracy: 0.9573 - val_loss: 2.0108 - val_digit1_loss: 0.3161 - val_digit2_loss: 0.3107 - val_digit3_loss: 0.2880 - val_digit4_loss: 0.3786 - val_digit5_loss: 0.3620 - val_digit6_loss: 0.3554 - val_digit1_accuracy: 0.9160 - val_digit2_accuracy: 0.9160 - val_digit3_accuracy: 0.9320 - val_digit4_accuracy: 0.9080 - val_digit5_accuracy: 0.9040 - val_digit6_accuracy: 0.9080\nEpoch 4/15\n36/36 - 7s - loss: 0.5854 - digit1_loss: 0.1040 - digit2_loss: 0.0896 - digit3_loss: 0.0767 - digit4_loss: 0.1046 - digit5_loss: 0.0991 - digit6_loss: 0.1114 - digit1_accuracy: 0.9651 - digit2_accuracy: 0.9713 - digit3_accuracy: 0.9713 - digit4_accuracy: 0.9664 - digit5_accuracy: 0.9669 - digit6_accuracy: 0.9631 - val_loss: 0.6935 - val_digit1_loss: 0.1062 - val_digit2_loss: 0.1003 - val_digit3_loss: 0.0770 - val_digit4_loss: 0.0994 - val_digit5_loss: 0.1388 - val_digit6_loss: 0.1717 - val_digit1_accuracy: 0.9640 - val_digit2_accuracy: 0.9640 - val_digit3_accuracy: 0.9760 - val_digit4_accuracy: 0.9700 - val_digit5_accuracy: 0.9580 - val_digit6_accuracy: 0.9500\nEpoch 5/15\n36/36 - 7s - loss: 0.4569 - digit1_loss: 0.0755 - digit2_loss: 0.0683 - digit3_loss: 0.0688 - digit4_loss: 0.0786 - digit5_loss: 0.0794 - digit6_loss: 0.0864 - digit1_accuracy: 0.9762 - digit2_accuracy: 0.9756 - digit3_accuracy: 0.9773 - digit4_accuracy: 0.9736 - digit5_accuracy: 0.9751 - digit6_accuracy: 0.9760 - val_loss: 0.8960 - val_digit1_loss: 0.1304 - val_digit2_loss: 0.1407 - val_digit3_loss: 0.1249 - val_digit4_loss: 0.1174 - val_digit5_loss: 0.1662 - val_digit6_loss: 0.2163 - val_digit1_accuracy: 0.9560 - val_digit2_accuracy: 0.9620 - val_digit3_accuracy: 0.9700 - val_digit4_accuracy: 0.9700 - val_digit5_accuracy: 0.9560 - val_digit6_accuracy: 0.9420\nEpoch 6/15\n36/36 - 7s - loss: 0.3976 - digit1_loss: 0.0688 - digit2_loss: 0.0647 - digit3_loss: 0.0589 - digit4_loss: 0.0702 - digit5_loss: 0.0675 - digit6_loss: 0.0674 - digit1_accuracy: 0.9760 - digit2_accuracy: 0.9780 - digit3_accuracy: 0.9798 - digit4_accuracy: 0.9778 - digit5_accuracy: 0.9793 - digit6_accuracy: 0.9760 - val_loss: 0.6142 - val_digit1_loss: 0.0826 - val_digit2_loss: 0.1068 - val_digit3_loss: 0.0626 - val_digit4_loss: 0.0976 - val_digit5_loss: 0.1204 - val_digit6_loss: 0.1442 - val_digit1_accuracy: 0.9740 - val_digit2_accuracy: 0.9620 - val_digit3_accuracy: 0.9820 - val_digit4_accuracy: 0.9720 - val_digit5_accuracy: 0.9680 - val_digit6_accuracy: 0.9580\nEpoch 7/15\n36/36 - 7s - loss: 0.3610 - digit1_loss: 0.0659 - digit2_loss: 0.0586 - digit3_loss: 0.0448 - digit4_loss: 0.0658 - digit5_loss: 0.0665 - digit6_loss: 0.0594 - digit1_accuracy: 0.9771 - digit2_accuracy: 0.9802 - digit3_accuracy: 0.9862 - digit4_accuracy: 0.9764 - digit5_accuracy: 0.9791 - digit6_accuracy: 0.9793 - val_loss: 0.7065 - val_digit1_loss: 0.1101 - val_digit2_loss: 0.1421 - val_digit3_loss: 0.0699 - val_digit4_loss: 0.1016 - val_digit5_loss: 0.1293 - val_digit6_loss: 0.1533 - val_digit1_accuracy: 0.9580 - val_digit2_accuracy: 0.9600 - val_digit3_accuracy: 0.9820 - val_digit4_accuracy: 0.9720 - val_digit5_accuracy: 0.9620 - val_digit6_accuracy: 0.9600\nEpoch 8/15\n36/36 - 7s - loss: 0.3350 - digit1_loss: 0.0513 - digit2_loss: 0.0483 - digit3_loss: 0.0479 - digit4_loss: 0.0585 - digit5_loss: 0.0692 - digit6_loss: 0.0598 - digit1_accuracy: 0.9847 - digit2_accuracy: 0.9856 - digit3_accuracy: 0.9838 - digit4_accuracy: 0.9811 - digit5_accuracy: 0.9787 - digit6_accuracy: 0.9833 - val_loss: 0.7278 - val_digit1_loss: 0.1013 - val_digit2_loss: 0.1212 - val_digit3_loss: 0.0837 - val_digit4_loss: 0.1148 - val_digit5_loss: 0.1300 - val_digit6_loss: 0.1768 - val_digit1_accuracy: 0.9620 - val_digit2_accuracy: 0.9660 - val_digit3_accuracy: 0.9760 - val_digit4_accuracy: 0.9720 - val_digit5_accuracy: 0.9620 - val_digit6_accuracy: 0.9500\nEpoch 9/15\n36/36 - 7s - loss: 0.2912 - digit1_loss: 0.0477 - digit2_loss: 0.0465 - digit3_loss: 0.0402 - digit4_loss: 0.0469 - digit5_loss: 0.0538 - digit6_loss: 0.0561 - digit1_accuracy: 0.9827 - digit2_accuracy: 0.9853 - digit3_accuracy: 0.9867 - digit4_accuracy: 0.9822 - digit5_accuracy: 0.9816 - digit6_accuracy: 0.9820 - val_loss: 0.7045 - val_digit1_loss: 0.1031 - val_digit2_loss: 0.1025 - val_digit3_loss: 0.0760 - val_digit4_loss: 0.1079 - val_digit5_loss: 0.1445 - val_digit6_loss: 0.1705 - val_digit1_accuracy: 0.9660 - val_digit2_accuracy: 0.9680 - val_digit3_accuracy: 0.9800 - val_digit4_accuracy: 0.9720 - val_digit5_accuracy: 0.9620 - val_digit6_accuracy: 0.9440\nEpoch 10/15\n36/36 - 7s - loss: 0.2796 - digit1_loss: 0.0444 - digit2_loss: 0.0405 - digit3_loss: 0.0435 - digit4_loss: 0.0443 - digit5_loss: 0.0527 - digit6_loss: 0.0543 - digit1_accuracy: 0.9864 - digit2_accuracy: 0.9871 - digit3_accuracy: 0.9871 - digit4_accuracy: 0.9862 - digit5_accuracy: 0.9838 - digit6_accuracy: 0.9818 - val_loss: 0.7726 - val_digit1_loss: 0.0967 - val_digit2_loss: 0.1423 - val_digit3_loss: 0.0842 - val_digit4_loss: 0.1205 - val_digit5_loss: 0.1458 - val_digit6_loss: 0.1831 - val_digit1_accuracy: 0.9640 - val_digit2_accuracy: 0.9640 - val_digit3_accuracy: 0.9720 - val_digit4_accuracy: 0.9620 - val_digit5_accuracy: 0.9620 - val_digit6_accuracy: 0.9500\nEpoch 11/15\n36/36 - 7s - loss: 0.2674 - digit1_loss: 0.0505 - digit2_loss: 0.0398 - digit3_loss: 0.0353 - digit4_loss: 0.0418 - digit5_loss: 0.0522 - digit6_loss: 0.0479 - digit1_accuracy: 0.9833 - digit2_accuracy: 0.9862 - digit3_accuracy: 0.9880 - digit4_accuracy: 0.9873 - digit5_accuracy: 0.9822 - digit6_accuracy: 0.9840 - val_loss: 0.8026 - val_digit1_loss: 0.1152 - val_digit2_loss: 0.1170 - val_digit3_loss: 0.0798 - val_digit4_loss: 0.1440 - val_digit5_loss: 0.1530 - val_digit6_loss: 0.1935 - val_digit1_accuracy: 0.9600 - val_digit2_accuracy: 0.9620 - val_digit3_accuracy: 0.9800 - val_digit4_accuracy: 0.9580 - val_digit5_accuracy: 0.9600 - val_digit6_accuracy: 0.9440\nEpoch 12/15\n36/36 - 7s - loss: 0.2680 - digit1_loss: 0.0413 - digit2_loss: 0.0423 - digit3_loss: 0.0364 - digit4_loss: 0.0431 - digit5_loss: 0.0559 - digit6_loss: 0.0490 - digit1_accuracy: 0.9867 - digit2_accuracy: 0.9860 - digit3_accuracy: 0.9891 - digit4_accuracy: 0.9871 - digit5_accuracy: 0.9820 - digit6_accuracy: 0.9827 - val_loss: 0.8727 - val_digit1_loss: 0.1140 - val_digit2_loss: 0.1531 - val_digit3_loss: 0.0866 - val_digit4_loss: 0.1205 - val_digit5_loss: 0.1822 - val_digit6_loss: 0.2163 - val_digit1_accuracy: 0.9620 - val_digit2_accuracy: 0.9520 - val_digit3_accuracy: 0.9680 - val_digit4_accuracy: 0.9600 - val_digit5_accuracy: 0.9460 - val_digit6_accuracy: 0.9460\nEpoch 13/15\n36/36 - 7s - loss: 0.2355 - digit1_loss: 0.0405 - digit2_loss: 0.0355 - digit3_loss: 0.0368 - digit4_loss: 0.0427 - digit5_loss: 0.0391 - digit6_loss: 0.0409 - digit1_accuracy: 0.9869 - digit2_accuracy: 0.9898 - digit3_accuracy: 0.9856 - digit4_accuracy: 0.9887 - digit5_accuracy: 0.9867 - digit6_accuracy: 0.9882 - val_loss: 0.7587 - val_digit1_loss: 0.1309 - val_digit2_loss: 0.1283 - val_digit3_loss: 0.0792 - val_digit4_loss: 0.0937 - val_digit5_loss: 0.1311 - val_digit6_loss: 0.1955 - val_digit1_accuracy: 0.9600 - val_digit2_accuracy: 0.9620 - val_digit3_accuracy: 0.9760 - val_digit4_accuracy: 0.9760 - val_digit5_accuracy: 0.9660 - val_digit6_accuracy: 0.9480\nEpoch 14/15\n36/36 - 7s - loss: 0.2014 - digit1_loss: 0.0357 - digit2_loss: 0.0300 - digit3_loss: 0.0280 - digit4_loss: 0.0335 - digit5_loss: 0.0358 - digit6_loss: 0.0384 - digit1_accuracy: 0.9887 - digit2_accuracy: 0.9896 - digit3_accuracy: 0.9902 - digit4_accuracy: 0.9889 - digit5_accuracy: 0.9876 - digit6_accuracy: 0.9864 - val_loss: 0.7372 - val_digit1_loss: 0.1116 - val_digit2_loss: 0.1112 - val_digit3_loss: 0.0790 - val_digit4_loss: 0.0925 - val_digit5_loss: 0.1466 - val_digit6_loss: 0.1962 - val_digit1_accuracy: 0.9660 - val_digit2_accuracy: 0.9700 - val_digit3_accuracy: 0.9760 - val_digit4_accuracy: 0.9780 - val_digit5_accuracy: 0.9620 - val_digit6_accuracy: 0.9540\nEpoch 15/15\n36/36 - 7s - loss: 0.2246 - digit1_loss: 0.0381 - digit2_loss: 0.0383 - digit3_loss: 0.0364 - digit4_loss: 0.0401 - digit5_loss: 0.0306 - digit6_loss: 0.0410 - digit1_accuracy: 0.9867 - digit2_accuracy: 0.9880 - digit3_accuracy: 0.9907 - digit4_accuracy: 0.9862 - digit5_accuracy: 0.9891 - digit6_accuracy: 0.9856 - val_loss: 0.9782 - val_digit1_loss: 0.1420 - val_digit2_loss: 0.1687 - val_digit3_loss: 0.1089 - val_digit4_loss: 0.1513 - val_digit5_loss: 0.1836 - val_digit6_loss: 0.2236 - val_digit1_accuracy: 0.9580 - val_digit2_accuracy: 0.9520 - val_digit3_accuracy: 0.9700 - val_digit4_accuracy: 0.9620 - val_digit5_accuracy: 0.9480 - val_digit6_accuracy: 0.9400\nEpoch 1/15\n36/36 - 7s - loss: 1.5870 - digit1_loss: 0.2865 - digit2_loss: 0.2439 - digit3_loss: 0.2230 - digit4_loss: 0.2496 - digit5_loss: 0.3231 - digit6_loss: 0.2610 - digit1_accuracy: 0.9173 - digit2_accuracy: 0.9307 - digit3_accuracy: 0.9349 - digit4_accuracy: 0.9247 - digit5_accuracy: 0.9129 - digit6_accuracy: 0.9218 - val_loss: 0.9069 - val_digit1_loss: 0.1654 - val_digit2_loss: 0.0953 - val_digit3_loss: 0.1787 - val_digit4_loss: 0.1706 - val_digit5_loss: 0.1232 - val_digit6_loss: 0.1737 - val_digit1_accuracy: 0.9520 - val_digit2_accuracy: 0.9680 - val_digit3_accuracy: 0.9540 - val_digit4_accuracy: 0.9500 - val_digit5_accuracy: 0.9660 - val_digit6_accuracy: 0.9520\nEpoch 2/15\n36/36 - 7s - loss: 0.8846 - digit1_loss: 0.1557 - digit2_loss: 0.1325 - digit3_loss: 0.1251 - digit4_loss: 0.1462 - digit5_loss: 0.1733 - digit6_loss: 0.1518 - digit1_accuracy: 0.9509 - digit2_accuracy: 0.9544 - digit3_accuracy: 0.9573 - digit4_accuracy: 0.9553 - digit5_accuracy: 0.9458 - digit6_accuracy: 0.9500 - val_loss: 0.8652 - val_digit1_loss: 0.1504 - val_digit2_loss: 0.1208 - val_digit3_loss: 0.1401 - val_digit4_loss: 0.1416 - val_digit5_loss: 0.1304 - val_digit6_loss: 0.1819 - val_digit1_accuracy: 0.9580 - val_digit2_accuracy: 0.9600 - val_digit3_accuracy: 0.9660 - val_digit4_accuracy: 0.9540 - val_digit5_accuracy: 0.9560 - val_digit6_accuracy: 0.9460\nEpoch 3/15\n36/36 - 7s - loss: 0.6474 - digit1_loss: 0.1160 - digit2_loss: 0.1028 - digit3_loss: 0.0836 - digit4_loss: 0.0984 - digit5_loss: 0.1299 - digit6_loss: 0.1167 - digit1_accuracy: 0.9651 - digit2_accuracy: 0.9660 - digit3_accuracy: 0.9724 - digit4_accuracy: 0.9669 - digit5_accuracy: 0.9596 - digit6_accuracy: 0.9613 - val_loss: 1.1232 - val_digit1_loss: 0.1769 - val_digit2_loss: 0.1556 - val_digit3_loss: 0.2119 - val_digit4_loss: 0.1876 - val_digit5_loss: 0.1662 - val_digit6_loss: 0.2249 - val_digit1_accuracy: 0.9440 - val_digit2_accuracy: 0.9480 - val_digit3_accuracy: 0.9400 - val_digit4_accuracy: 0.9400 - val_digit5_accuracy: 0.9440 - val_digit6_accuracy: 0.9300\nEpoch 4/15\n36/36 - 7s - loss: 0.5142 - digit1_loss: 0.0999 - digit2_loss: 0.0759 - digit3_loss: 0.0717 - digit4_loss: 0.0798 - digit5_loss: 0.0995 - digit6_loss: 0.0874 - digit1_accuracy: 0.9684 - digit2_accuracy: 0.9722 - digit3_accuracy: 0.9747 - digit4_accuracy: 0.9736 - digit5_accuracy: 0.9691 - digit6_accuracy: 0.9727 - val_loss: 0.7606 - val_digit1_loss: 0.1387 - val_digit2_loss: 0.0812 - val_digit3_loss: 0.1554 - val_digit4_loss: 0.1090 - val_digit5_loss: 0.1087 - val_digit6_loss: 0.1676 - val_digit1_accuracy: 0.9600 - val_digit2_accuracy: 0.9720 - val_digit3_accuracy: 0.9540 - val_digit4_accuracy: 0.9660 - val_digit5_accuracy: 0.9720 - val_digit6_accuracy: 0.9480\nEpoch 5/15\n36/36 - 7s - loss: 0.3982 - digit1_loss: 0.0726 - digit2_loss: 0.0606 - digit3_loss: 0.0632 - digit4_loss: 0.0716 - digit5_loss: 0.0707 - digit6_loss: 0.0596 - digit1_accuracy: 0.9782 - digit2_accuracy: 0.9771 - digit3_accuracy: 0.9782 - digit4_accuracy: 0.9751 - digit5_accuracy: 0.9769 - digit6_accuracy: 0.9816 - val_loss: 0.8626 - val_digit1_loss: 0.1457 - val_digit2_loss: 0.0843 - val_digit3_loss: 0.1533 - val_digit4_loss: 0.1757 - val_digit5_loss: 0.1201 - val_digit6_loss: 0.1834 - val_digit1_accuracy: 0.9600 - val_digit2_accuracy: 0.9720 - val_digit3_accuracy: 0.9640 - val_digit4_accuracy: 0.9560 - val_digit5_accuracy: 0.9640 - val_digit6_accuracy: 0.9460\nEpoch 6/15\n36/36 - 7s - loss: 0.3836 - digit1_loss: 0.0676 - digit2_loss: 0.0602 - digit3_loss: 0.0552 - digit4_loss: 0.0574 - digit5_loss: 0.0776 - digit6_loss: 0.0656 - digit1_accuracy: 0.9789 - digit2_accuracy: 0.9793 - digit3_accuracy: 0.9847 - digit4_accuracy: 0.9820 - digit5_accuracy: 0.9756 - digit6_accuracy: 0.9780 - val_loss: 1.0447 - val_digit1_loss: 0.1658 - val_digit2_loss: 0.1406 - val_digit3_loss: 0.1604 - val_digit4_loss: 0.1961 - val_digit5_loss: 0.1608 - val_digit6_loss: 0.2209 - val_digit1_accuracy: 0.9500 - val_digit2_accuracy: 0.9600 - val_digit3_accuracy: 0.9580 - val_digit4_accuracy: 0.9440 - val_digit5_accuracy: 0.9600 - val_digit6_accuracy: 0.9460\nEpoch 7/15\n36/36 - 7s - loss: 0.3520 - digit1_loss: 0.0501 - digit2_loss: 0.0511 - digit3_loss: 0.0541 - digit4_loss: 0.0668 - digit5_loss: 0.0694 - digit6_loss: 0.0605 - digit1_accuracy: 0.9824 - digit2_accuracy: 0.9827 - digit3_accuracy: 0.9809 - digit4_accuracy: 0.9793 - digit5_accuracy: 0.9778 - digit6_accuracy: 0.9800 - val_loss: 1.1658 - val_digit1_loss: 0.1798 - val_digit2_loss: 0.1618 - val_digit3_loss: 0.1933 - val_digit4_loss: 0.2321 - val_digit5_loss: 0.1868 - val_digit6_loss: 0.2119 - val_digit1_accuracy: 0.9480 - val_digit2_accuracy: 0.9400 - val_digit3_accuracy: 0.9500 - val_digit4_accuracy: 0.9340 - val_digit5_accuracy: 0.9600 - val_digit6_accuracy: 0.9400\nEpoch 8/15\n36/36 - 7s - loss: 0.3087 - digit1_loss: 0.0587 - digit2_loss: 0.0468 - digit3_loss: 0.0384 - digit4_loss: 0.0472 - digit5_loss: 0.0671 - digit6_loss: 0.0505 - digit1_accuracy: 0.9798 - digit2_accuracy: 0.9836 - digit3_accuracy: 0.9871 - digit4_accuracy: 0.9849 - digit5_accuracy: 0.9793 - digit6_accuracy: 0.9816 - val_loss: 0.9539 - val_digit1_loss: 0.1779 - val_digit2_loss: 0.1143 - val_digit3_loss: 0.1137 - val_digit4_loss: 0.2152 - val_digit5_loss: 0.1445 - val_digit6_loss: 0.1882 - val_digit1_accuracy: 0.9540 - val_digit2_accuracy: 0.9660 - val_digit3_accuracy: 0.9660 - val_digit4_accuracy: 0.9360 - val_digit5_accuracy: 0.9600 - val_digit6_accuracy: 0.9460\nEpoch 9/15\n36/36 - 7s - loss: 0.3210 - digit1_loss: 0.0632 - digit2_loss: 0.0513 - digit3_loss: 0.0454 - digit4_loss: 0.0521 - digit5_loss: 0.0491 - digit6_loss: 0.0601 - digit1_accuracy: 0.9793 - digit2_accuracy: 0.9838 - digit3_accuracy: 0.9829 - digit4_accuracy: 0.9822 - digit5_accuracy: 0.9831 - digit6_accuracy: 0.9820 - val_loss: 1.1866 - val_digit1_loss: 0.2143 - val_digit2_loss: 0.1692 - val_digit3_loss: 0.1858 - val_digit4_loss: 0.1779 - val_digit5_loss: 0.2158 - val_digit6_loss: 0.2236 - val_digit1_accuracy: 0.9420 - val_digit2_accuracy: 0.9580 - val_digit3_accuracy: 0.9460 - val_digit4_accuracy: 0.9460 - val_digit5_accuracy: 0.9400 - val_digit6_accuracy: 0.9260\nEpoch 10/15\n36/36 - 7s - loss: 0.2799 - digit1_loss: 0.0439 - digit2_loss: 0.0377 - digit3_loss: 0.0360 - digit4_loss: 0.0504 - digit5_loss: 0.0636 - digit6_loss: 0.0484 - digit1_accuracy: 0.9851 - digit2_accuracy: 0.9882 - digit3_accuracy: 0.9876 - digit4_accuracy: 0.9829 - digit5_accuracy: 0.9796 - digit6_accuracy: 0.9836 - val_loss: 0.8493 - val_digit1_loss: 0.1623 - val_digit2_loss: 0.0932 - val_digit3_loss: 0.1397 - val_digit4_loss: 0.1516 - val_digit5_loss: 0.1470 - val_digit6_loss: 0.1556 - val_digit1_accuracy: 0.9580 - val_digit2_accuracy: 0.9700 - val_digit3_accuracy: 0.9560 - val_digit4_accuracy: 0.9600 - val_digit5_accuracy: 0.9640 - val_digit6_accuracy: 0.9540\nEpoch 11/15\n36/36 - 7s - loss: 0.2540 - digit1_loss: 0.0452 - digit2_loss: 0.0339 - digit3_loss: 0.0400 - digit4_loss: 0.0379 - digit5_loss: 0.0451 - digit6_loss: 0.0519 - digit1_accuracy: 0.9853 - digit2_accuracy: 0.9907 - digit3_accuracy: 0.9887 - digit4_accuracy: 0.9876 - digit5_accuracy: 0.9858 - digit6_accuracy: 0.9829 - val_loss: 0.9810 - val_digit1_loss: 0.2140 - val_digit2_loss: 0.1091 - val_digit3_loss: 0.1709 - val_digit4_loss: 0.1601 - val_digit5_loss: 0.1598 - val_digit6_loss: 0.1672 - val_digit1_accuracy: 0.9540 - val_digit2_accuracy: 0.9660 - val_digit3_accuracy: 0.9540 - val_digit4_accuracy: 0.9500 - val_digit5_accuracy: 0.9560 - val_digit6_accuracy: 0.9480\nEpoch 12/15\n36/36 - 7s - loss: 0.2091 - digit1_loss: 0.0391 - digit2_loss: 0.0313 - digit3_loss: 0.0246 - digit4_loss: 0.0388 - digit5_loss: 0.0360 - digit6_loss: 0.0393 - digit1_accuracy: 0.9864 - digit2_accuracy: 0.9907 - digit3_accuracy: 0.9933 - digit4_accuracy: 0.9876 - digit5_accuracy: 0.9887 - digit6_accuracy: 0.9869 - val_loss: 1.0719 - val_digit1_loss: 0.1962 - val_digit2_loss: 0.1613 - val_digit3_loss: 0.1713 - val_digit4_loss: 0.1752 - val_digit5_loss: 0.1662 - val_digit6_loss: 0.2017 - val_digit1_accuracy: 0.9440 - val_digit2_accuracy: 0.9640 - val_digit3_accuracy: 0.9460 - val_digit4_accuracy: 0.9460 - val_digit5_accuracy: 0.9540 - val_digit6_accuracy: 0.9280\nEpoch 13/15\n36/36 - 7s - loss: 0.2225 - digit1_loss: 0.0299 - digit2_loss: 0.0385 - digit3_loss: 0.0331 - digit4_loss: 0.0437 - digit5_loss: 0.0385 - digit6_loss: 0.0390 - digit1_accuracy: 0.9913 - digit2_accuracy: 0.9882 - digit3_accuracy: 0.9896 - digit4_accuracy: 0.9871 - digit5_accuracy: 0.9858 - digit6_accuracy: 0.9876 - val_loss: 1.0776 - val_digit1_loss: 0.1605 - val_digit2_loss: 0.1764 - val_digit3_loss: 0.1599 - val_digit4_loss: 0.2028 - val_digit5_loss: 0.1890 - val_digit6_loss: 0.1891 - val_digit1_accuracy: 0.9580 - val_digit2_accuracy: 0.9560 - val_digit3_accuracy: 0.9600 - val_digit4_accuracy: 0.9440 - val_digit5_accuracy: 0.9520 - val_digit6_accuracy: 0.9340\nEpoch 14/15\n36/36 - 7s - loss: 0.2025 - digit1_loss: 0.0419 - digit2_loss: 0.0321 - digit3_loss: 0.0206 - digit4_loss: 0.0331 - digit5_loss: 0.0435 - digit6_loss: 0.0315 - digit1_accuracy: 0.9878 - digit2_accuracy: 0.9907 - digit3_accuracy: 0.9924 - digit4_accuracy: 0.9904 - digit5_accuracy: 0.9882 - digit6_accuracy: 0.9896 - val_loss: 0.8886 - val_digit1_loss: 0.1388 - val_digit2_loss: 0.1279 - val_digit3_loss: 0.1314 - val_digit4_loss: 0.1414 - val_digit5_loss: 0.1601 - val_digit6_loss: 0.1890 - val_digit1_accuracy: 0.9600 - val_digit2_accuracy: 0.9680 - val_digit3_accuracy: 0.9600 - val_digit4_accuracy: 0.9640 - val_digit5_accuracy: 0.9620 - val_digit6_accuracy: 0.9460\nEpoch 15/15\n36/36 - 7s - loss: 0.1852 - digit1_loss: 0.0312 - digit2_loss: 0.0328 - digit3_loss: 0.0276 - digit4_loss: 0.0326 - digit5_loss: 0.0271 - digit6_loss: 0.0339 - digit1_accuracy: 0.9904 - digit2_accuracy: 0.9898 - digit3_accuracy: 0.9927 - digit4_accuracy: 0.9898 - digit5_accuracy: 0.9900 - digit6_accuracy: 0.9893 - val_loss: 1.0452 - val_digit1_loss: 0.1763 - val_digit2_loss: 0.2139 - val_digit3_loss: 0.1373 - val_digit4_loss: 0.1774 - val_digit5_loss: 0.1629 - val_digit6_loss: 0.1774 - val_digit1_accuracy: 0.9520 - val_digit2_accuracy: 0.9520 - val_digit3_accuracy: 0.9520 - val_digit4_accuracy: 0.9440 - val_digit5_accuracy: 0.9580 - val_digit6_accuracy: 0.9480\n"
    }
   ],
   "source": [
    "epochs=10\n",
    "batch_size=128\n",
    "big_batch_size=5000\n",
    "train_data_gen=iter(train_data02.repeat().batch(big_batch_size))\n",
    "\n",
    "for i in range(int(num_elements/big_batch_size)):\n",
    "   img , lb = next(train_data_gen)\n",
    "   history = model.fit(\n",
    "       img,\n",
    "       lb,\n",
    "       batch_size=batch_size,\n",
    "       validation_split=0.1,\n",
    "       shuffle=True,\n",
    "       use_multiprocessing=True,\n",
    "       epochs=epochs,\n",
    "       verbose=2\n",
    "       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseResult(RS):\n",
    "    for j in range(len(RS[0])):\n",
    "        PredChr=''\n",
    "        for i in range(6):\n",
    "            PredChr+= CLASS_NAMES[(np.where(RS[i][j] == RS[i][j].max())[0][0])]\n",
    "        if j == 0:\n",
    "            LB = [PredChr]\n",
    "        else:\n",
    "            LB.append(PredChr)      \n",
    "    return LB\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output to CSV\n",
    "VDlabel = np.array([])\n",
    "img = next(iter(val_data01.batch(10000)))\n",
    "result = model.predict(img,use_multiprocessing = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dir = tf.data.Dataset.list_files(val_dir_data01+'*.jpg',shuffle=False) \n",
    "test = parseResult(result)\n",
    "val_file_name = pd.read_csv(\"./dev/data01_dev.csv\")\n",
    "val_file_name.join(pd.DataFrame(test,columns=[\"code\"])).to_csv(\"data01_dev.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('data01_20200521.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitmyenvcondac89683127ca741bc84ed6db3ffcad32d",
   "display_name": "Python 3.7.7 64-bit ('myenv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}