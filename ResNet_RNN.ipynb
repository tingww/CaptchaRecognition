{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar zxvf ./drive/My\\ Drive/Colab\\ Notebooks/train.tgz\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAPTCHA Recognition - 6 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = np.array([])\n",
    "for i in range(10):\n",
    "    CLASS_NAMES =np.append(CLASS_NAMES,chr(ord(\"0\")+i))\n",
    "for i in range(26):\n",
    "    CLASS_NAMES = np.append(CLASS_NAMES,chr(ord(\"A\")+i))\n",
    "\n",
    "#functions\n",
    "def parse_label(strIN):\n",
    "    return (strIN==CLASS_NAMES).astype(float)\n",
    "def readimg_to_tensor(fn):\n",
    "    a = tf.io.read_file((fn))\n",
    "    img = tf.io.decode_jpeg(a)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data 01 \n",
    "train_dir_data01 = os.path.abspath(os.getcwd())+\"/train/data01_train/\"\n",
    "data = pd.read_csv(\"./train/data01_train.csv\")\n",
    "for i in range(len(data)):\n",
    "    data.iloc[i,1] = list(data.iloc[i,1])\n",
    "arr = np.zeros([6,50000],str)\n",
    "for j in range(6):\n",
    "    for i in range(len(data)):\n",
    "        arr[j][i] = data.iloc[i,1][j]\n",
    "data = data.drop(columns= [\"code\"]).join(pd.DataFrame(arr.transpose(),columns = [\"code0\",\"code1\",'code2','code3','code4','code5']))\n",
    "\n",
    "for i in range(6):\n",
    "    data.iloc[:,i+1] = data.iloc[:,i+1].map(parse_label)\n",
    "\n",
    "TL = tf.convert_to_tensor(list(data.iloc[:,1].values))\n",
    "TL2 = tf.convert_to_tensor(list(data.iloc[:,2].values))\n",
    "TL3 = tf.convert_to_tensor(list(data.iloc[:,3].values))\n",
    "TL4 = tf.convert_to_tensor(list(data.iloc[:,4].values))\n",
    "TL5 = tf.convert_to_tensor(list(data.iloc[:,5].values))\n",
    "TL6 = tf.convert_to_tensor(list(data.iloc[:,6].values))\n",
    "\n",
    "train_dir = tf.data.Dataset.list_files(train_dir_data01+'*.jpg',shuffle=False) \n",
    "train_data = train_dir.map(lambda x: readimg_to_tensor(x))\n",
    "train_label = tf.data.Dataset.from_tensor_slices((TL,TL2,TL3,TL4,TL5,TL6))\n",
    "train_data01=tf.data.Dataset.zip((train_data,train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data 02\n",
    "train_dir_data02 = os.path.abspath(os.getcwd())+\"/train/data02_train/\"\n",
    "data2 = pd.read_csv(\"./train/data02_train.csv\")\n",
    "for i in range(len(data2)):\n",
    "    data2.iloc[i,1] = list(data2.iloc[i,1])\n",
    "arr = np.zeros([6,50000],str)\n",
    "for j in range(6):\n",
    "    for i in range(len(data2)):\n",
    "        arr[j][i] = data2.iloc[i,1][j]\n",
    "data2 = data2.drop(columns= [\"code\"]).join(pd.DataFrame(arr.transpose(),columns = [\"code0\",\"code1\",'code2','code3','code4','code5']))\n",
    "\n",
    "#to one hot\n",
    "for i in range(6):\n",
    "    data2.iloc[:,i+1] = data2.iloc[:,i+1].map(parse_label)\n",
    "\n",
    "TL = tf.convert_to_tensor(list(data2.iloc[:,1].values))\n",
    "TL2 = tf.convert_to_tensor(list(data2.iloc[:,2].values))\n",
    "TL3 = tf.convert_to_tensor(list(data2.iloc[:,3].values))\n",
    "TL4 = tf.convert_to_tensor(list(data2.iloc[:,4].values))\n",
    "TL5 = tf.convert_to_tensor(list(data2.iloc[:,5].values))\n",
    "TL6 = tf.convert_to_tensor(list(data2.iloc[:,6].values))\n",
    "\n",
    "train_dir2 = tf.data.Dataset.list_files(train_dir_data02+'*.jpg',shuffle=False) \n",
    "train_data2 = train_dir2.map(lambda x: readimg_to_tensor(x))\n",
    "train_label2 = tf.data.Dataset.from_tensor_slices((TL,TL2,TL3,TL4,TL5,TL6))\n",
    "train_data02=tf.data.Dataset.zip((train_data2,train_label2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of element\n",
    "num_elements = tf.data.experimental.cardinality(train_data01).numpy()\n",
    "num_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepData01(): \n",
    "  validation_split=0.1\n",
    "  split = int(num_elements*validation_split)\n",
    "  train_data_gen=iter(train_data01.shuffle(10000).batch(10000))\n",
    "\n",
    "  img , lb = next(train_data_gen)\n",
    "  #img_train = img[:-split]\n",
    "  #lb_train = (lb[0][:-split],lb[1][:-split],lb[2][:-split],lb[3][:-split],lb[4][:-split],lb[5][:-split])\n",
    "  #img_test = img[-split:]\n",
    "  #lb_test = (lb[0][-split:],lb[1][-split:],lb[2][-split:],lb[3][-split:],lb[4][-split:],lb[5][-split:])\n",
    "  return img,lb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model configeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ini and regul\n",
    "initializer = tf.keras.initializers.he_normal( seed = 3)\n",
    "alpha = 0.0001  # weight decay coefficient 0.001 Fail\n",
    "regularizer = tf.keras.regularizers.l2(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre activate\n",
    "def block2(x, filters, kernel_size=3, stride=1, conv_shortcut=True, name=None):\n",
    "  \"\"\"A residual block.\n",
    "  Arguments:\n",
    "      x: input tensor.\n",
    "      filters: integer, filters of the bottleneck layer.\n",
    "      kernel_size: default 3, kernel size of the bottleneck layer.\n",
    "      stride: default 1, stride of the first layer.\n",
    "      conv_shortcut: default False, use convolution shortcut if True,\n",
    "        otherwise identity shortcut.\n",
    "      name: string, block label.\n",
    "  Returns:\n",
    "    Output tensor for the residual block.\n",
    "  \"\"\"\n",
    "  bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
    "\n",
    "  preact = layers.BatchNormalization(\n",
    "      axis=bn_axis, epsilon=1.001e-5, name=name + '_preact_bn')(x)\n",
    "  preact = layers.Activation('relu', name=name + '_preact_relu')(preact)\n",
    "\n",
    "  if conv_shortcut:\n",
    "    shortcut = layers.Conv2D(\n",
    "        4 * filters, 1, strides=stride, name=name + '_0_conv', kernel_initializer = initializer,kernel_regularizer = regularizer)(x)\n",
    "  else:\n",
    "    shortcut = layers.MaxPooling2D(1, strides=stride)(x) if stride > 1 else x\n",
    "\n",
    "  x = layers.Conv2D(\n",
    "      filters, 1, strides=1, use_bias=False, name=name + '_1_conv', kernel_initializer = initializer,kernel_regularizer = regularizer)(preact)\n",
    "  x = layers.BatchNormalization(\n",
    "      axis=bn_axis, epsilon=1.001e-5, name=name + '_1_bn')(x)\n",
    "  x = layers.Activation('relu', name=name + '_1_relu')(x)\n",
    "\n",
    "  x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name=name + '_2_pad')(x)\n",
    "  x = layers.Conv2D(\n",
    "      filters,\n",
    "      kernel_size,\n",
    "      strides=stride,\n",
    "      use_bias=False,\n",
    "      name=name + '_2_conv', kernel_initializer = initializer,kernel_regularizer = regularizer)(x)\n",
    "  x = layers.BatchNormalization(\n",
    "      axis=bn_axis, epsilon=1.001e-5, name=name + '_2_bn')(x)\n",
    "  x = layers.Activation('relu', name=name + '_2_relu')(x)\n",
    "\n",
    "  x = layers.Conv2D(4 * filters, 1, name=name + '_3_conv', kernel_initializer = initializer,kernel_regularizer = regularizer)(x)\n",
    "  x = layers.Dropout(0.3)(x)\n",
    "  x = layers.Add(name=name + '_out')([shortcut, x])\n",
    "  return x\n",
    "\n",
    "def stack1(x, filters, blocks, stride1=2, name=None):\n",
    "  \"\"\"A set of stacked residual blocks.\n",
    "  Arguments:\n",
    "    x: input tensor.\n",
    "    filters: integer, filters of the bottleneck layer in a block.\n",
    "    blocks: integer, blocks in the stacked blocks.\n",
    "    stride1: default 2, stride of the first layer in the first block.\n",
    "    name: string, stack label.\n",
    "  Returns:\n",
    "    Output tensor for the stacked blocks.\n",
    "  \"\"\"\n",
    "  x = block2(x, filters, conv_shortcut=True ,stride=stride1, name=name + '_block1')\n",
    "  for i in range(2, blocks + 1):\n",
    "    x = block2(x, filters, conv_shortcut=False, name=name + '_block' + str(i))\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(modelname=\"myResNet\"):\n",
    "  bn_axis = 3 \n",
    "  #ResNet+RNN config\n",
    "  def stack_fn(x):\n",
    "    x = stack1(x, 64, 3, stride1=1, name='conv2')\n",
    "    x = stack1(x, 128, 4, name='conv3')\n",
    "    return stack1(x, 256, 3, name='conv4')\n",
    "  #input shape\n",
    "  img_input = layers.Input(shape=(60,200,3))\n",
    "\n",
    "  x = layers.ZeroPadding2D(\n",
    "      padding=((1, 1), (1, 1)), name='conv1_pad')(img_input)\n",
    "  x = layers.Conv2D(32, 3, strides=2, use_bias=True, name='conv1_conv',kernel_initializer = initializer,kernel_regularizer = regularizer)(x)\n",
    "  #pre act\n",
    "  x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name='pool1_pad')(x)\n",
    "  x = layers.MaxPooling2D(3, strides=2, name='pool1_pool')(x)\n",
    "  x = layers.Dropout(0.3)(x)\n",
    "  x = stack_fn(x)\n",
    "  \n",
    "  x = layers.BatchNormalization(\n",
    "        axis=bn_axis, epsilon=1.001e-5, name='post_bn')(x)\n",
    "  x = layers.Activation('relu', name='post_relu')(x)\n",
    "  x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "  #RNN\n",
    "  x = layers.RepeatVector(6)(x)\n",
    "  x = layers.GRU(128,return_sequences=True)(x)\n",
    "  x = layers.GRU(128,return_sequences=True)(x)\n",
    "  #classify\n",
    "  digit1 = layers.Dense(36, name='digit1', activation='softmax',kernel_initializer = initializer,kernel_regularizer = regularizer)(x)\n",
    "  #digit2 = layers.Dense(36, name='digit2', activation='softmax')(x)\n",
    "  #digit3 = layers.Dense(36, name='digit3', activation='softmax')(x)\n",
    "  #digit4 = layers.Dense(36, name='digit4', activation='softmax')(x)\n",
    "  #digit5 = layers.Dense(36, name='digit5', activation='softmax')(x)\n",
    "  #digit6 = layers.Dense(36, name='digit6', activation='softmax')(x)\n",
    "  modelt = tf.keras.models.Model(inputs=img_input,outputs = digit1, name = modelname)\n",
    "  modelt.compile(\n",
    "    loss=[\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "        tf.keras.losses.CategoricalCrossentropy(),\n",
    "    ],\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9), \n",
    "    metrics=['accuracy'])\n",
    "  return modelt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = buildModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img , lb= prepData01()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = tf.concat([tf.expand_dims(lbs,1) for lbs in lb],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model,img_train,lb_train,callB):\n",
    "  history = model.fit(\n",
    "        img_train,\n",
    "        lb_train,\n",
    "        batch_size = 32,\n",
    "        shuffle = True,\n",
    "        validation_split=0.1,\n",
    "        use_multiprocessing=True,\n",
    "        epochs=100,\n",
    "        verbose=1,\n",
    "        callbacks = callB\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"stack\" : \"32+ 64,128,256 3,4,3blocks\",\n",
    "    \"weight_regu\" : 0.0001,\n",
    "    \"dropout\" : 0.3,\n",
    "    \"preact\" : True,\n",
    "    \"name\" : \"ResNet_RNN\",\n",
    "    \"output_WR\" : 1,\n",
    "    \"total_params\" : 5741604\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"my-project-2\",reinit=True,name=config[\"name\"], config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"h5s/\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath+config[\"name\"]+\".hdf5\", monitor='val_loss', verbose=1, save_best_only=True,mode='min')\n",
    "trainModel(mymodel,img,lb,[checkpoint, wandb.keras.WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.save(\"./h5s/DenseNet_no_train.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitmyenvcondac89683127ca741bc84ed6db3ffcad32d",
   "display_name": "Python 3.7.7 64-bit ('myenv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}